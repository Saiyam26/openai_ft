0	What is the seed lexicon?
1	What are the results?
2	How are relations used to propagate polarity?
3	How big is the Japanese data?
4	What are labels available in dataset for supervision?
5	How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?
6	How does their model learn using mostly raw data?
7	How big is seed lexicon used for training?
8	How large is raw corpus used for training?
9	Does the paper report macro F1?
10	How is the annotation experiment evaluated?
11	What are the aesthetic emotions formalized?
12	Do they report results only on English data?
13	How do the various social phenomena examined manifest in different types of communities?
14	What patterns do they observe about how user engagement varies with the characteristics of a community?
15	How did the select the 300 Reddit communities for comparison?
16	How do the authors measure how temporally dynamic a community is?
17	How do the authors measure how distinctive a community is?
18	What data is the language model pretrained on?
19	What baselines is the proposed model compared against?
20	How is the clinical text structuring task defined?
21	What are the specific tasks being unified?
22	Is all text in this dataset a question, or are there unrelated sentences in between questions?
23	How many questions are in the dataset?
24	What is the perWhat are the tasks evaluated?
25	Are there privacy concerns with clinical data?
26	How they introduce domain-specific features into pre-trained language model?
27	How big is QA-CTS task dataset?
28	How big is dataset of pathology reports collected from Ruijing Hospital?
29	What are strong baseline models in specific tasks?
30	What aspects have been compared between various language models?
31	what classic language models are mentioned in the paper?
32	What is a commonly used evaluation metric for language models?
33	Which dataset do they use a starting point in generating fake reviews?
34	Do they use a pretrained NMT model to help generating reviews?
35	How does using NMT ensure generated reviews stay on topic?
36	What kind of model do they use for detection?
37	Does their detection tool work better than human detection?
38	How many reviews in total (both generated and true) do they evaluate on Amazon Mechanical Turk?
39	Which baselines did they compare?
40	How many attention layers are there in their model?
41	Is the explanation from saliency map correct?
42	How is embedding quality assessed?
43	What are the three measures of bias which are reduced in experiments?
44	What are the probabilistic observations which contribute to the more robust algorithm?
45	What turn out to be more important high volume or high quality data?
46	How much is model improved by massive data and how much by quality?
47	What two architectures are used?
48	Does this paper target European or Brazilian Portuguese?
49	What were the word embeddings trained on?
50	Which word embeddings are analysed?
51	Did they experiment on this dataset?
52	How is quality of the citation measured?
53	How big is the dataset?
54	Do they evaluate only on English datasets?
55	Do the authors mention any possible confounds in this study?
56	How is the intensity of the PTSD established?
57	How is LIWC incorporated into this system?
58	How many twitter users are surveyed using the clinically validated survey?
59	Which clinically validated survey tools are used?
60	Did they experiment with the dataset?
61	What is the size of this dataset?
62	Do they list all the named entity types present?
63	how is quality measured?
64	how many languages exactly is the sentiment lexica for?
65	what sentiment sources do they compare with?
66	Is the method described in this work a clustering-based method?
67	How are the different senses annotated/labeled? 
68	Was any extrinsic evaluation carried out?
69	Does the model use both spectrogram images and raw waveforms as features?
70	Is the performance compared against a baseline model?
71	What is the accuracy reported by state-of-the-art methods?
72	Which vision-based approaches does this approach outperform?
73	What baseline is used for the experimental setup?
74	Which languages are used in the multi-lingual caption model?
75	Did they experiment on all the tasks?
76	What models did they compare to?
77	What datasets are used in training?
78	Which GAN do they use?
79	Do they evaluate grammaticality of generated text?
80	Which corpora do they use?
81	Do they report results only on English datasets?
82	How do the authors define or exemplify 'incorrect words'?
83	How many vanilla transformers do they use after applying an embedding layer?
84	Do they test their approach on a dataset without incomplete data?
85	Should their approach be applied only when dealing with incomplete data?
86	By how much do they outperform other models in the sentiment in intent classification tasks?
87	What is the sample size of people used to measure user satisfaction?
88	What are all the metrics to measure user engagement?
89	What the system designs introduced?
90	Do they specify the model they use for Gunrock?
91	Do they gather explicit user satisfaction data on Gunrock?
92	How do they correlate user backstory queries to user satisfaction?
93	Do the authors report only on English?
94	What is the baseline for the experiments?
95	Which experiments are perfomed?
96	Is ROUGE their only baseline?
97	what language models do they use?
98	what questions do they ask human judges?
99	What misbehavior is identified?
100	What is the baseline used?
101	Which attention mechanisms do they compare?
102	Which paired corpora did they use in the other experiment?
103	By how much does their system outperform the lexicon-based models?
104	Which lexicon-based models did they compare with?
105	How many comments were used?
106	How many articles did they have?
107	What news comment dataset was used?
108	By how much do they outperform standard BERT?
109	What dataset do they use?
110	How do they combine text representations with the knowledge graph embeddings?
111	What is the algorithm used for the classification tasks?
112	Is the outcome of the LDA analysis evaluated in any way?
113	What is the corpus used in the study?
114	What are the traditional methods to identifying important attributes?
115	What do you use to calculate word/sub-word embeddings
116	What user generated text data do you use?
117	Did they propose other metrics?
118	Which real-world datasets did they use?
119	How did they obtain human intuitions?
120	What are the country-specific drivers of international development rhetoric?
121	Is the dataset multilingual?
122	How are the main international development topics that states raise identified?
123	What experiments do the authors present to validate their system?
124	How does the conversation layer work?
125	What components is the QnAMaker composed of?
126	How they measure robustness in experiments?
127	Is new method inferior in terms of robustness to MIRAs in experiments?
128	What experiments with large-scale features are performed?
129	Which ASR system(s) is used in this work?
130	What are the series of simple models?
131	Over which datasets/corpora is this work evaluated?
132	Is the semantic hierarchy representation used for any task?
133	What are the corpora used for the task?
134	Is the model evaluated?
135	What new metrics are suggested to track progress?
136	What intrinsic evaluation metrics are used?
137	What experimental results suggest that using less than 50% of the available training examples might result in overfitting?
138	What multimodality is available in the dataset?
139	What are previously reported models?
140	How better is accuracy of new model compared to previously reported models?
141	How does the scoring model work?
142	How does the active learning model work?
143	Which neural network architectures are employed?
144	What are the key points in the role of script knowledge that can be studied?
145	Did the annotators agreed and how much?
146	How many subjects have been used to create the annotations?
147	What datasets are used to evaluate this approach?
148	How is this approach used to detect incorrect facts?
149	Can this adversarial approach be used to directly improve model accuracy?
150	what are the advantages of the proposed model?
151	what are the state of the art approaches?
152	what datasets were used?
153	How was the dataset collected?
154	What are the benchmark models?
155	How was the corpus annotated?
156	What models other than standalone BERT is new model compared to?
157	How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?
158	What are three downstream task datasets?
159	What is dataset for word probing task?
160	How fast is the model compared to baselines?
161	How big is the performance difference between this method and the baseline?
162	What datasets used for evaluation?
163	what are the mentioned cues?
164	How did the author's work rank among other submissions on the challenge?
165	What approaches without reinforcement learning have been tried?
166	What classification approaches were experimented for this task?
167	Did classification models perform better than previous regression one?
168	What are the main sources of recall errors in the mapping?
169	Do they look for inconsistencies between different languages' annotations in UniMorph?
170	Do they look for inconsistencies between different UD treebanks?
171	Which languages do they validate on?
172	Does the paper evaluate any adjustment to improve the predicion accuracy of face and audio features?
173	How is face and audio data analysis evaluated?
174	What is the baseline method for the task?
175	What are the emotion detection tools used for audio and face input?
176	what amounts of size were used on german-english?
177	what were their experimental results in the low-resource dataset?
178	what are the methods they compare with in the korean-english dataset?
179	what pitfalls are mentioned in the paper?
180	Does the paper report the results of previous models applied to the same tasks?
181	How is the quality of the discussion evaluated?
182	What is the technique used for text analysis and mining?
183	What are the causal mapping methods employed?
184	What is the previous work's model?
185	What dataset is used?
186	How big is the dataset?
187	How is the dataset collected?
188	Was each text augmentation technique experimented individually?
189	What models do previous work use?
190	Does the dataset contain content from various social media platforms?
191	What dataset is used?
192	How they demonstrate that language-neutral component is sufficiently general in terms of modeling semantics to allow high-accuracy word-alignment?
193	Are language-specific and language-neutral components disjunctive?
194	How they show that mBERT representations can be split into a language-specific component and a language-neutral component?
195	What challenges this work presents that must be solved to build better language-neutral representations?
196	What is the performance of their system?
197	What evaluation metrics are used?
198	What is the source of the dialogues?
199	What pretrained LM is used?
200	What approaches they propose?
201	What faithfulness criteria does they propose?
202	Which are three assumptions in current approaches for defining faithfulness?
203	Which are key points in guidelines for faithfulness evaluation?
204	Did they use the state-of-the-art model to analyze the attention?
205	What is the performance of their model?
206	How many layers are there in their model?
207	Did they compare with gradient-based methods?
208	What MC abbreviate for?
209	how much of improvement the adaptation model can get?
210	what is the architecture of the baseline model?
211	What is the exact performance on SQUAD?
212	What are their correlation results?
213	What dataset do they use?
214	What simpler models do they look at?
215	What linguistic quality aspects are addressed?
216	What benchmark datasets are used for the link prediction task?
217	What are state-of-the art models for this task?
218	How better does HAKE model peform than state-of-the-art methods?
219	How are entities mapped onto polar coordinate system?
220	What additional techniques are incorporated?
221	What dataset do they use?
222	Do they compare to other models?
223	What is the architecture of the system?
224	How long are expressions in layman's language?
225	What additional techniques could be incorporated to further improve accuracy?
226	What programming language is target language?
227	What dataset is used to measure accuracy?
228	Is text-to-image synthesis trained is suppervized or unsuppervized manner?
229	What challenges remain unresolved?
230	What is the conclusion of comparison of proposed solution?
231	What is typical GAN architecture for each text-to-image synhesis group?
232	Where do they employ feature-wise sigmoid gating?
233	Which model architecture do they use to obtain representations?
234	Which downstream sentence-level tasks do they evaluate on?
235	Which similarity datasets do they use?
236	Are there datasets with relation tuples annotated, how big are datasets available?
237	Which one of two proposed approaches performed better in experiments?
238	What is previous work authors reffer to?
239	How higher are F1 scores compared to previous work?
240	what were the baselines?
241	what is the supervised model they developed?
242	what is the size of this built corpus?
243	what crowdsourcing platform is used?
244	Which deep learning model performed better?
245	By how much did the results improve?
246	What was their performance on the dataset?
247	How large is the dataset?
248	Did the authors use crowdsourcing platforms?
249	How was the dataset collected?
250	What language do the agents talk in?
251	What evaluation metrics did the authors look at?
252	What data did they use?
253	Do the authors report results only on English data?
254	How is the accuracy of the system measured?
255	How is an incoming claim used to retrieve similar factchecked claims?
256	What existing corpus is used for comparison in these experiments?
257	What are the components in the factchecking algorithm? 
258	What is the baseline?
259	What dataset was used in the experiment?
260	Did they use any crowdsourcing platform?
261	How was the dataset annotated?
262	What is the source of the proposed dataset?
263	How many label options are there in the multi-label task?
264	What is the interannotator agreement of the crowd sourced users?
265	Who are the experts?
266	Who is the crowd in these experiments?
267	How do you establish the ground truth of who won a debate?
268	How much better is performance of proposed method than state-of-the-art methods in experiments?
269	What further analysis is done?
270	What seven state-of-the-art methods are used for comparison?
271	What three datasets are used to measure performance?
272	How does KANE capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner?
273	What are recent works on knowedge graph embeddings authors mention?
274	Do they report results only on English data?
275	Do the authors mention any confounds to their study?
276	What baseline model is used?
277	What stylistic features are used to detect drunk texts?
278	Is the data acquired under distant supervision verified by humans at any stage?
279	What hashtags are used for distant supervision?
280	Do the authors equate drunk tweeting with drunk texting? 
281	What corpus was the source of the OpenIE extractions?
282	What is the accuracy of the proposed technique?
283	Is an entity linking process used?
284	Are the OpenIE extractions all triples?
285	What method was used to generate the OpenIE extractions?
286	Can the method answer multi-hop questions?
287	What was the textual source to which OpenIE was applied?
288	What OpenIE method was used to generate the extractions?
289	Is their method capable of multi-hop reasoning?
290	Do the authors offer any hypothesis about why the dense mode outperformed the sparse one?
291	What evaluation is conducted?
292	Which corpus of synsets are used?
293	What measure of semantic similarity is used?
294	Which retrieval system was used for baselines?
295	What word embeddings were used?
296	What type of errors were produced by the BLSTM-CNN-CRF system?
297	How much better was the BLSTM-CNN-CRF than the BLSTM-CRF?
298	What supplemental tasks are used for multitask learning?
299	Is the improvement actually coming from using an RNN?
300	How much performance gap between their approach and the strong handcrafted method?
301	What is a strong feature-based method?
302	Did they experimnet in other languages?
303	Do they use multi-attention heads?
304	How big is their model?
305	How is their model different from BERT?
306	What datasets were used?
307	How did they do compared to other teams?
308	Which tested technique was the worst performer?
309	How many emotions do they look at?
310	What are the baseline benchmarks?
311	What is the size of this dataset?
312	How many annotators were there?
313	Can SCRF be used to pretrain the model?
314	What conclusions are drawn from the syntactic analysis?
315	What type of syntactic analysis is performed?
316	How is it demonstrated that the correct gender and number information is injected using this system?
317	Which neural machine translation system is used?
318	What are the components of the black-box context injection system?
319	What normalization techniques are mentioned?
320	What features do they experiment with?
321	Which architecture is their best model?
322	What kind of spontaneous speech is used?
323	What approach did previous models use for multi-span questions?
324	How they use sequence tagging to answer multi-span questions?
325	What is difference in peformance between proposed model and state-of-the art on other question types?
326	What is the performance of proposed model on entire DROP dataset?
327	What is the previous model that attempted to tackle multi-span questions as a part of its design?
328	How much more data does the model trained using XR loss have access to, compared to the fully supervised model?
329	Does the system trained only using XR loss outperform the fully supervised neural system?
330	How accurate is the aspect based sentiment classifier trained only using the XR loss?
331	How is the expectation regularization loss defined?
332	What were the non-neural baselines used for the task?
333	Which publicly available NLU dataset is used?
334	What metrics other than entity tagging are compared?
335	Do they provide decision sequences as supervision while training models?
336	What are the models evaluated on?
337	How do they train models in this setup?
338	What commands does their setup provide to models seeking information?
339	What models do they propose?
340	Are all tweets in English?
341	How large is the dataset?
342	What is the results of multimodal compared to unimodal models?
343	What is author's opinion on why current multimodal models cannot outperform models analyzing only text?
344	What metrics are used to benchmark the results?
345	How is data collected, manual collection or Twitter api?
346	How many tweats does MMHS150k contains, 150000?
347	What unimodal detection models were used?
348	What different models for multimodal detection were proposed?
349	What annotations are available in the dataset - tweat used hate speach or not?
350	What were the evaluation metrics used?
351	What were their performance results?
352	By how much did they outperform the other methods?
353	Which popular clustering methods did they experiment with?
354	What datasets did they use?
355	Does pre-training on general text corpus improve performance?
356	What neural configurations are explored?
357	Are the Transformers masked?
358	How is this problem evaluated?
359	What datasets do they use?
360	What evaluation metrics were used?
361	Where did the real production data come from?
362	What feedback labels are used?
363	What representations for textual documents do they use?
364	Which dataset(s) do they use?
365	How do they evaluate knowledge extraction performance?
366	What is CamemBERT trained on?
367	Which tasks does CamemBERT not improve on?
368	What is the state of the art?
369	How much better was results of CamemBERT than previous results on these tasks?
370	Was CamemBERT compared against multilingual BERT on these tasks?
371	How long was CamemBERT trained?
372	What data is used for training CamemBERT?
373	What are the state of the art measures?
374	What controversial topics are experimented with?
375	What datasets did they use?
376	What social media platform is observed?
377	How many languages do they experiment with?
378	What is the current SOTA for sentiment analysis on Twitter at the time of writing?
379	What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?
380	What are the metrics to evaluate sentiment analysis on Twitter?
381	How many sentence transformations on average are available per unique sentence in dataset?
382	What annotations are available in the dataset?
383	How are possible sentence transformations represented in dataset, as new sentences?
384	What are all 15 types of modifications ilustrated in the dataset?
385	Is this dataset publicly available?
386	Are some baseline models trained on this dataset?
387	Do they do any analysis of of how the modifications changed the starting set of sentences?
388	How do they introduce language variation?
389	Do they use external resources to make modifications to sentences?
390	How big is dataset domain-specific embedding are trained on?
391	How big is unrelated corpus universal embedding is traned on?
392	How better are state-of-the-art results than this model? 
393	What were their results on the three datasets?
394	What was the baseline?
395	Which datasets did they use?
396	Are results reported only on English datasets?
397	Which three Twitter sentiment classification datasets are used for experiments?
398	What semantic rules are proposed?
399	Which knowledge graph completion tasks do they experiment with?
400	Apart from using desired properties, do they evaluate their LAN approach in some other way?
401	Do they evaluate existing methods in terms of desired properties?
402	How does the model differ from Generative Adversarial Networks?
403	What is the dataset used to train the model?
404	What is the performance of the model?
405	Is the model evaluated against a CNN baseline?
406	Does the model proposed beat the baseline models for all the values of the masking parameter tested?
407	Has STES been previously used in the literature to evaluate similar tasks?
408	What are the baseline models mentioned in the paper?
409	What was the performance of both approaches on their dataset?
410	What kind of settings do the utterances come from?
411	What genres are covered?
412	Do they experiment with cross-genre setups?
413	Which of the two speech recognition models works better overall on CN-Celeb?
414	By how much is performance on CN-Celeb inferior to performance on VoxCeleb?
415	On what datasets is the new model evaluated on?
416	How do the authors measure performance?
417	Does the new objective perform better than the original objective bert is trained on?
418	Are other pretrained language models also evaluated for contextual augmentation? 
419	Do the authors report performance of conditional bert on tasks without data augmentation?
420	Do they cover data augmentation papers?
421	What is the latest paper covered by this survey?
422	Do they survey visual question generation work?
423	Do they survey multilingual aspects?
424	What learning paradigms do they cover in this survey?
425	What are all the input modalities considered in prior work in question generation?
426	Do they survey non-neural methods for question generation?
427	What is their model?
428	Do they evaluate on NER data sets?
429	What previously proposed methods is this method compared against?
430	How is effective word score calculated?
431	How is tweet subjectivity measured?
432	Why is supporting fact supervision necessary for DMN?
433	What does supporting fact supervision mean?
434	What changes they did on input module?
435	What improvements they did for DMN?
436	How does the model circumvent the lack of supporting facts during training?
437	Does the DMN+ model establish state-of-the-art ?
438	Is this style generator compared to some baseline?
439	How they perform manual evaluation, what is criteria?
440	What metrics are used for automatic evaluation?
441	How they know what are content words?
442	How they model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions?
443	Do they report results only on English data?
444	What insights into the relationship between demographics and mental health are provided?
445	What model is used to achieve 5% improvement on F1 for identifying depressed individuals on Twitter?
446	How do this framework facilitate demographic inference from social media?
447	What types of features are used from each data type?
448	How is the data annotated?
449	Where does the information on individual-level demographics come from?
450	What is the source of the user interaction data? 
451	What is the source of the textual data? 
452	What is the source of the visual data? 
453	Is there an online demo of their system?
454	Do they perform manual evaluation?
455	Do they compare against Noraset et al. 2017?
456	What is a sememe?
457	What data did they use?
458	What is the state of the art?
459	What language tasks did they experiment on?
460	What result from experiments suggest that natural language based agents are more robust?
461	How better is performance of natural language based agents in experiments?
462	How much faster natural language agents converge in performed experiments?
463	What experiments authors perform?
464	How is state to learn and complete tasks represented via natural language?
465	How does the model compare with the MMR baseline?
466	Does the paper discuss previous models which have been applied to the same task?
467	Which datasets are used in the paper?
468	How does the parameter-free model work?
469	How do they quantify moral relevance?
470	Which fine-grained moral dimension examples do they showcase?
471	Which dataset sources to they use to demonstrate moral sentiment through history?
472	How well did the system do?
473	How is the information extracted?
474	What are some guidelines in writing input vernacular so model can generate 
475	How much is proposed model better in perplexity and BLEU score than typical UMT models?
476	What dataset is used for training?
477	What were the evaluation metrics?
478	What were the baseline systems?
479	Which dialog datasets did they experiment with?
480	What KB is used?
481	At which interval do they extract video and audio frames?
482	Do they use pretrained word vectors for dialogue context embedding?
483	Do they train a different training method except from scheduled sampling?
484	Is the web interface publicly accessible?
485	Is the Android application publicly available?
486	What classifier is used for emergency categorization?
487	What classifier is used for emergency detection?
488	Do the tweets come from any individual?
489	How many categories are there?
490	What was the baseline?
491	Are the tweets specific to a region?
492	Do they release MED?
493	What NLI models do they analyze?
494	How do they define upward and downward reasoning?
495	What is monotonicity reasoning?
496	What other relations were found in the datasets?
497	How does the ensemble annotator extract the final label?
498	How were dialogue act labels defined?
499	How many models were used?
500	Do they compare their neural network against any other model?
501	Do they annotate their own dataset or use an existing one?
502	Does their neural network predict a single offset in a recording?
503	What kind of neural network architecture do they use?
504	How are aspects identified in aspect extraction?
505	What web and user-generated NER datasets are used for the analysis?
506	Which unlabeled data do they pretrain with?
507	How many convolutional layers does their model have?
508	Do they explore how much traning data is needed for which magnitude of improvement for WER? 
509	How are character representations from various languages joint?
510	On which dataset is the experiment conducted?
511	Do they train their own RE model?
512	How big are the datasets?
513	What languages do they experiment on?
514	What datasets are used?
515	How better does auto-completion perform when using both language and vision than only language?
516	How big is data provided by this research?
517	How they complete a user query prefix conditioned upon an image?
518	Did the collection process use a WoZ method?
519	By how much did their model outperform the baseline?
520	What baselines did they compare their model with?
521	What was the performance of their model?
522	What evaluation metrics are used?
523	Did the authors use a crowdsourcing platform?
524	How were the navigation instructions collected?
525	What language is the experiment done in?
526	What additional features are proposed for future work?
527	What are their initial results on this task?
528	What datasets did the authors use?
529	How many linguistic and semantic features are learned?
530	How is morphology knowledge implemented in the method?
531	How does the word segmentation method work?
532	Is the word segmentation method independently evaluated?
533	Do they normalize the calculated intermediate output hypotheses to compensate for the incompleteness?
534	How many layers do they use in their best performing network?
535	Do they just sum up all the loses the calculate to end up with one single loss?
536	Does their model take more time to train than regular transformer models?
537	Are agglutinative languages used in the prediction of both prefixing and suffixing languages?
538	What is an example of a prefixing language?
539	How is the performance on the task evaluated?
540	What are the tree target languages studied in the paper?
541	Is the model evaluated against any baseline?
542	Does the paper report the accuracy of the model?
543	How is the performance of the model evaluated?
544	What are the different bilingual models employed?
545	How does the well-resourced language impact the quality of the output?
546	what are the baselines?
547	did they outperform previous methods?
548	what language pairs are explored?
549	what datasets were used?
550	How is order of binomials tracked across time?
551	What types of various community texts have been investigated for exploring global structure of binomials?
552	Are there any new finding in analasys of trinomials that was not present binomials?
553	What new model is proposed for binomial lists?
554	How was performance of previously proposed rules at very large scale?
555	What previously proposed rules for predicting binoial ordering are used?
556	What online text resources are used to test binomial lists?
557	How do they model a city description using embeddings?
558	How do they obtain human judgements?
559	Which clustering method do they use to cluster city description embeddings?
560	Does this approach perform better in the multi-domain or single-domain setting?
561	What are the performance metrics used?
562	Which datasets are used to evaluate performance?
563	How does the automatic theorem prover infer the relation?
564	If these model can learn the first-order logic on artificial language, why can't it lear for natural language?
565	How many samples did they generate for the artificial language?
566	Which of their training domains improves performance the most?
567	Do they fine-tune their model on the end task?
568	Why does not the approach from English work on other languages?
569	How do they measure grammaticality?
570	Which model do they use to convert between masculine-inflected and feminine-inflected sentences?
571	What is the performance achieved by the model described in the paper?
572	What is the best performance achieved by supervised models?
573	What is the size of the datasets employed?
574	What are the baseline models?
575	What evaluation metrics are used?
576	What datasets did they use?
577	Why does their model do better than prior models?
578	What is the difference in recall score between the systems?
579	What is their f1 score and recall?
580	How many layers does their system have?
581	Which news corpus is used?
582	How large is the dataset they used?
583	Which coreference resolution systems are tested?
584	How big is improvement in performances of proposed model over state of the art?
585	What two large datasets are used for evaluation?
586	What context modelling methods are evaluated?
587	What are two datasets models are tested on?
588	How big is the improvement over the state-of-the-art results?
589	Is the model evaluated against other Aspect-Based models?
590	Is the baseline a non-heirarchical model like BERT?
591	Do they build a model to recognize discourse relations on their dataset?
592	Which inter-annotator metric do they use?
593	How high is the inter-annotator agreement?
594	How are resources adapted to properties of Chinese text?
595	How better are results compared to baseline models?
596	What models that rely only on claim-specific linguistic features are used as baselines?
597	How is pargmative and discourse context added to the dataset?
598	What annotations are available in the dataset?
599	How big is dataset used for training/testing?
600	Is there any example where geometric property is visible for context similarity between words?
601	What geometric properties do embeddings display?
602	How accurate is model trained on text exclusively?
603	What was their result on Stance Sentiment Emotion Corpus?
604	What performance did they obtain on the SemEval dataset?
605	What are the state-of-the-art systems?
606	How is multi-tasking performed?
607	What are the datasets used for training?
608	How many parameters does the model have?
609	What is the previous state-of-the-art model?
610	What is the previous state-of-the-art performance?
611	How can the classifier facilitate the annotation task for human annotators?
612	What recommendations are made to improve the performance in future?
613	What type of errors do the classifiers use?
614	What neural classifiers are used?
615	What is the hashtags does the hashtag-based baseline use?
616	What languages are included in the dataset?
617	What dataset is used for this study?
618	What proxies for data annotation were used in previous datasets?
619	What are the supported natural commands?
620	What is the size of their collected dataset?
621	Did they compare against other systems?
622	What intents does the paper explore?
623	What kind of features are used by the HMM models, and how interpretable are those?
624	What kind of information do the HMMs learn that the LSTMs don't?
625	Which methods do the authors use to reach the conclusion that LSTMs and HMMs learn complementary information?
626	How large is the gap in performance between the HMMs and the LSTMs?
627	Do they report results only on English data?
628	Which publicly available datasets are used?
629	What embedding algorithm and dimension size are used?
630	What data are the embeddings trained on?
631	how much was the parameter difference between their model and previous methods?
632	how many parameters did their model use?
633	which datasets were used?
634	what was their system's f1 performance?
635	what was the baseline?
636	What datasets were used?
637	What language pairs did they experiment with?
638	How much more coverage is in the new dataset?
639	How was coverage measured?
640	How was quality measured?
641	How was the corpus obtained?
642	How are workers trained?
643	What is different in the improved annotation protocol?
644	How was the previous dataset annotated?
645	How big is the dataset?
646	Do the other multilingual baselines make use of the same amount of training data?
647	How big is the impact of training data size on the performance of the multilingual encoder?
648	What data were they used to train the multilingual encoder?
649	From when are many VQA datasets collected?
650	What is task success rate achieved? 
651	What simulations are performed by the authors to validate their approach?
652	Does proposed end-to-end approach learn in reinforcement or supervised learning manner?
653	Are synonymous relation taken into account in the Japanese-Vietnamese task?
654	Is the supervised morphological learner tested on Japanese?
655	What is the dataset that is used in the paper?
656	What is the performance of the models discussed in the paper?
657	Does the paper consider the use of perplexity in order to identify text anomalies?
658	Does the paper report a baseline for the task?
659	What non-contextual properties do they refer to?
660	What is the baseline?
661	What are their proposed features?
662	What are overall baseline results on new this new task?
663	What metrics are used in evaluation of this task?
664	Do authors provide any explanation for intriguing patterns of word being echoed?
665	What features are proposed?
666	Which datasets are used to train this model?
667	How is performance of this system measured?
668	How many questions per image on average are available in dataset?
669	Is machine learning system underneath similar to image caption ML systems?
670	How big dataset is used for training this system?
671	How do they obtain word lattices from words?
672	Which metrics do they use to evaluate matching?
673	Which dataset(s) do they evaluate on?
674	What languages do they look at?
675	Do they report results only on English data?
676	Do they propose any further additions that could be made to improve generalisation to unseen speakers?
677	What are the characteristics of the dataset?
678	What type of models are used for classification?
679	Do they compare to previous work?
680	How many instances does their dataset have?
681	What model do they use to classify phonetic segments? 
682	How many speakers do they have in the dataset?
683	How better is proposed method than baselines perpexity wise?
684	How does the multi-turn dialog system learns?
685	How is human evaluation performed?
686	Is some other metrics other then perplexity measured?
687	What two baseline models are used?
688	Do they evaluate on relation extraction?
689	What is the baseline model for the agreement-based mode?
690	Do the authors suggest why syntactic parsing is so important for semantic role labelling for interlanguages?
691	Who manually annotated the semantic roles for the set of learner texts?
692	By how much do they outperform existing state-of-the-art VQA models?
693	How do they measure the correlation between manual groundings and model generated ones?
694	How do they obtain region descriptions and object annotations?
695	Which training dataset allowed for the best generalization to benchmark sets?
696	Which model generalized the best?
697	Which models were compared?
698	Which datasets were used?
699	What was the baseline?
700	Is the data all in Vietnamese?
701	What classifier do they use?
702	What is private dashboard?
703	What is public dashboard?
704	What dataset do they use?
705	Do the authors report results only on English data?
706	What other interesting correlations are observed?
707	what were the baselines?
708	what is the state of the art for ranking mc test answers?
709	what is the size of the introduced dataset?
710	what datasets did they use?
711	What evaluation metric is used?
712	What datasets are used?
713	What are three main machine translation tasks?
714	How big is improvement in performance over Transformers?
715	How do they determine the opinion summary?
716	Do they explore how useful is the detection history and opinion summary?
717	Which dataset(s) do they use to train the model?
718	By how much do they outperform state-of-the-art methods?
719	What is the average number of turns per dialog?
720	What baseline models are offered?
721	Which six domains are covered in the dataset?
722	What other natural processing tasks authors think could be studied by using word embeddings?
723	What is the reason that traditional co-occurrence networks fail in establishing links between similar words whenever they appear distant in the text?
724	Do the use word embeddings alone or they replace some previous features of the model with word embeddings?
725	On what model architectures are previous co-occurence networks based?
726	Is model explanation output evaluated, what metric was used?
727	How many annotators are used to write natural language explanations to SNLI-VE-2.0?
728	How many natural language explanations are human-written?
729	How much is performance difference of existing model between original and corrected corpus?
730	What is the class with highest error rate in SNLI-VE?
731	What is the dataset used as input to the Word2Vec algorithm?
732	Are the word embeddings tested on a NLP task?
733	Are the word embeddings evaluated?
734	How big is dataset used to train Word2Vec for the Italian Language?
735	How does different parameter settings impact the performance and semantic capacity of resulting model?
736	Are the semantic analysis findings for Italian language similar to English language version?
737	What dataset is used for training Word2Vec in Italian language?
738	How are the auxiliary signals from the morphology table incorporated in the decoder?
739	What type of morphological information is contained in the "morphology table"?
740	Do they report results only on English data?
741	Do the authors mention any confounds to their study?
742	Which machine learning models are used?
743	What methodology is used to compensate for limited labelled data?
744	Which five natural disasters were examined?
745	Which social media platform is explored?
746	What datasets did they use?
747	What are the baseline state of the art models?
748	What is the size of the dataset?
749	What model did they use?
750	What patterns were discovered from the stories?
751	Did they use a crowdsourcing platform?
752	Does the performance increase using their method?
753	What tasks are they experimenting with in this paper?
754	What is the size of the open vocabulary?
755	How do they select answer candidates for their QA task?
756	How do they extract causality from text?
757	What is the source of the "control" corpus?
758	What are the selection criteria for "causal statements"?
759	Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?
760	how do they collect the comparable corpus?
761	How do they collect the control corpus?
762	What languages do they experiment with?
763	What are the baselines?
764	What was the inter-annotator agreement?
765	Did they use a crowdsourcing platform?
766	Are resolution mode variables hand crafted?
767	What are resolution model variables?
768	Is the model presented in the paper state of the art?
769	What problems are found with the evaluation scheme?
770	How is the data annotated?
771	What collection steps do they mention?
772	How many intents were classified?
773	What was the result of the highest performing system?
774	What metrics are used in the evaluation?
775	How do they measure the quality of summaries?
776	Does their model also take the expected answer style as input?
777	What do they mean by answer styles?
778	Is there exactly one "answer style" per dataset?
779	What are the baselines that Masque is compared against?
780	What is the performance achieved on NarrativeQA?
781	What is an "answer style"?
782	What was the previous state of the art model for this task?
783	What syntactic structure is used to model tones?
784	What visual information characterizes tones?
785	Do they report results only on English data?
786	How do they demonstrate the robustness of their results?
787	What baseline and classification systems are used in experiments?
788	How are the EAU text spans annotated?
789	How are elementary argumentative units defined?
790	Which Twitter corpus was used to train the word vectors?
791	How does proposed word embeddings compare to Sindhi fastText word representations?
792	Are trained word embeddings used for any other NLP task?
793	How many uniue words are in the dataset?
794	How is the data collected, which web resources were used?
795	What trends are found in musical preferences?
796	Which decades did they look at?
797	How many genres did they collect from?
798	Does the paper mention other works proposing methods to detect anglicisms in Spanish?
799	What is the performance of the CRF model on the task described?
800	Does the paper motivate the use of CRF as the baseline model?
801	What are the handcrafted features used?
802	What is state of the art method?
803	By how much do proposed architectures autperform state-of-the-art?
804	What are three new proposed architectures?
805	How much does the standard metrics for style accuracy vary on different re-runs?
806	Which baseline methods are used?
807	How much is the BLEU score?
808	Which datasets are used in experiments?
809	What regularizers were used to encourage consistency in back translation cycles?
810	What are new best results on standard benchmark?
811	How better is performance compared to competitive baselines?
812	How big is data used in experiments?
813	What 6 language pairs is experimented on?
814	What are current state-of-the-art methods that consider the two tasks independently?
815	How big is their training set?
816	What baseline do they compare to?
817	Which pre-trained transformer do they use?
818	What is the FEVER task?
819	How is correctness of automatic derivation proved?
820	Is this AD implementation used in any deep learning framework?
821	Do they conduct any human evaluation?
822	What dataset do they use for experiments?
823	How do they enrich the positional embedding with length information
824	How do they condition the output to a given target-source class?
825	Which languages do they focus on?
826	What dataset do they use?
827	Do they experiment with combining both methods?
828	What state-of-the-art models are compared against?
829	Does API provide ability to connect to models written in some other deep learning framework?
830	Is this library implemented into Torch or is framework agnostic?
831	What baselines are used in experiments?
832	What general-purpose optimizations are included?
833	what baseline do they compare to?
834	How does this compare to traditional calibration methods like Platt Scaling?
835	What's the input representation of OpenIE tuples into the model?
836	What statistics on the VIST dataset are reported?
837	What is the performance difference in performance in unsupervised feature learning between adverserial training and FHVAE-based disentangled speech represenation learning?
838	What are puns?
839	What are the categories of code-mixed puns?
840	How is dialogue guided to avoid interactions that breach procedures and processes only known to experts?
841	What is meant by semiguided dialogue, what part of dialogue is guided?
842	Is CRWIZ already used for data collection, what are the results?
843	How does framework made sure that dialogue will not breach procedures?
844	How do they combine the models?
845	What is their baseline?
846	What context do they use?
847	What is their definition of hate speech?
848	What architecture has the neural network?
849	How is human interaction consumed by the model?
850	How do they evaluate generated stories?
851	Do they evaluate in other language appart from English?
852	What are the baselines?
853	What is used a baseline?
854	What contextual features are used?
855	Where are the cybersecurity articles used in the model sourced from?
856	What type of hand-crafted features are used in state of the art IOC detection systems?
857	Do they compare DeepER against other approaches?
858	How is the data in RAFAEL labelled?
859	How do they handle polysemous words in their entity library?
860	How is the fluctuation in the sense of the word and its neighbors measured?
861	Among various transfer learning techniques, which technique yields to the best performance?
862	What is the architecture of the model?
863	What language do they look at?
864	Where does the vocabulary come from?
865	What is the worst performing translation granularity?
866	What dataset did they use?
867	How do they measure performance?
868	Do they measure the performance of a combined approach?
869	Which four QA systems do they use?
870	How many iterations of visual search are done on average until an answer is found?
871	Do they test performance of their approaches using human judgements?
872	What are the sizes of both datasets?
873	Why are the scores for predicting perceived musical hardness and darkness extracted only for subsample of 503 songs?
874	How long is the model trained?
875	What are lyrical topics present in the metal genre?
876	By how much does SPNet outperforms state-of-the-art abstractive summarization methods on evaluation metrics?
877	What automatic and human evaluation metrics are used to compare SPNet to its counterparts?
878	Is proposed abstractive dialog summarization dataset open source?
879	Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?
880	How does SPNet utilize additional speaker role, semantic slot and dialog domain annotations?
881	What are previous state-of-the-art document summarization methods used?
882	How does new evaluation metric considers critical informative entities?
883	Is new evaluation metric extension of ROGUE?
884	What measures were used for human evaluation?
885	What automatic metrics are used for this task?
886	Are the models required to also generate rationales?
887	Are the rationales generated after the sentences were written?
888	Are the sentences in the dataset written by humans who were shown the concept-sets?
889	Where do the concept sets come from?
890	How big are improvements of MMM over state of the art?
891	What out of domain datasets authors used for coarse-tuning stage?
892	What are state of the art methods MMM is compared to?
893	What four representative datasets are used for bechmark?
894	What baselines did they consider?
895	What are the problems related to ambiguity in PICO sentence prediction tasks?
896	How is knowledge retrieved in the memory?
897	How is knowledge stored in the memory?
898	What are the relative improvements observed over existing methods?
899	What is the architecture of the neural network?
900	What methods is RelNet compared to?
901	How do they measure the diversity of inferences?
902	By how much do they improve the accuracy of inferences over state-of-the-art methods?
903	Which models do they use as baselines on the Atomic dataset?
904	How does the context-aware variational autoencoder learn event background information?
905	What is the size of the Atomic dataset?
906	what standard speech transcription pipeline was used?
907	How much improvement does their method get over the fine tuning baseline?
908	What kinds of neural networks did they use in this paper?
909	How did they use the domain tags?
910	Why mixed initiative multi-turn dialogs are the greatest challenge in building open-domain conversational agents?
911	How is speed measured?
912	What is the architecture of their model?
913	What are the nine types?
914	How do they represent input features of their model to train embeddings?
915	Which dimensionality do they use for their embeddings?
916	Which dataset do they use?
917	By how much do they outpeform previous results on the word discrimination task?
918	How does Frege's holistic and functional approach to meaning relates to general distributional hypothesis?
919	What does Frege's holistic and functional approach to meaning states?
920	Do they evaluate the quality of the paraphrasing model?
921	How many paraphrases are generated per question?
922	What latent variables are modeled in the PCFG?
923	What are the baselines?
924	Do they evaluate only on English data?
925	How strong was the correlation between exercise and diabetes?
926	How were topics of interest about DDEO identified?
927	What datasets are used for evaluation?
928	How do their train their embeddings?
929	How do they model travel behavior?
930	How do their interpret the coefficients?
931	By how much do they outperform previous state-of-the-art models?
932	Do they use pretrained word embeddings?
933	How is the lexicon of trafficking flags expanded?
934	Do they experiment with the dataset?
935	Do they use a crowdsourcing platform for annotation?
936	What is an example of a difficult-to-classify case?
937	What potential solutions are suggested?
938	What is the size of the dataset?
939	What Reddit communities do they look at?
940	How strong is negative correlation between compound divergence and accuracy in performed experiment?
941	What are results of comparison between novel method to other approaches for creating compositional generalization benchmarks?
942	How authors justify that question answering dataset presented is realistic?
943	What three machine architectures are analyzed?
944	How big is new question answering dataset?
945	What are other approaches into creating compositional generalization benchmarks?
946	What problem do they apply transfer learning to?
947	What are the baselines?
948	What languages are considered?
949	Does this model train faster than state of the art models?
950	What is the performance difference between proposed method and state-of-the-arts on these datasets?
951	What non autoregressive NMT models are used for comparison?
952	What are three neural machine translation (NMT) benchmark datasets used for evaluation?
953	What is result of their attention distribution analysis?
954	What is result of their Principal Component Analysis?
955	What are 3 novel fusion techniques that are proposed?
956	What are two models' architectures in proposed solution?
957	How do two models cooperate to select the most confident chains?
958	How many hand-labeled reasoning chains have been created?
959	What benchmarks are created?
960	What empricial investigations do they reference?
961	What languages do they investigate for machine translation?
962	What recommendations do they offer?
963	What percentage fewer errors did professional translations make?
964	What was the weakness in Hassan et al's evaluation design?
965	By how much they improve over the previous state-of-the-art?
966	Is there any evidence that encoders with latent structures work well on other tasks?
967	Do they report results only on English?
968	What evidence do they present that the model attends to shallow context clues?
969	In what way is the input restructured?
970	What are their results on the entity recognition task?
971	What task-specific features are used?
972	What kind of corpus-based features are taken into account?
973	Which machine learning algorithms did the explore?
974	What language is the Twitter content in?
975	What is the architecture of the siamese neural network?
976	How do they explore domain mismatch?
977	How do they explore dialect variability?
978	Which are the four Arabic dialects?
979	What factors contribute to interpretive biases according to this research?
980	Which interpretative biases are analyzed in this paper?
981	How many words are coded in the dictionary?
982	Is the model evaluated on the graphemes-to-phonemes task?
983	How does the QuaSP+Zero model work?
984	Which off-the-shelf tools do they use on QuaRel?
985	How do they obtain the logical forms of their questions in their dataset?
986	Do all questions in the dataset allow the answers to pick from 2 options?
987	What is shared in the joint model?
988	Are the intent labels imbalanced in the dataset?
989	What kernels are used in the support vector machines?
990	What dataset is used?
991	What metrics are considered?
992	Did the authors evaluate their system output for coherence?
993	What evaluations did the authors use on their system?
994	What accuracy does CNN model achieve?
995	How many documents are in the Indiscapes dataset?
996	What language(s) are the manuscripts written in?
997	What metrics are used to evaluation revision detection?
998	How large is the Wikipedia revision dump dataset?
999	What are simulated datasets collected?
1000	Which are the state-of-the-art models?
1001	Why is being feature-engineering free an advantage?
1002	Where did this model place in the final evaluation of the shared task?
1003	What in-domain data is used to continue pre-training?
1004	What dialect is used in the Google BERT model and what is used in the task data?
1005	What are the tasks used in the mulit-task learning setup?
1006	What human evaluation metrics were used in the paper?
1007	For the purposes of this paper, how is something determined to be domain specific knowledge?
1008	Does the fact that GCNs can perform well on this tell us that the task is simpler than previously thought?
1009	Are there conceptual benefits to using GCNs over more complex architectures like attention?
1010	Do they evaluate only on English?
1011	What are the 7 different datasets?
1012	What are the three different sources of data?
1013	What type of model are the ELMo representations used in?
1014	Which morphosyntactic features are thought to indicate irony or sarcasm?
1015	Is the model evaluated on other datasets?
1016	Does the model incorporate coreference and entailment?
1017	Is the incorporation of context separately evaluated?
1018	Which frozen acoustic model do they use?
1019	By how much does using phonetic feedback improve state-of-the-art systems?
1020	What features are used?
1021	Do they compare to previous models?
1022	How do they incorporate sentiment analysis?
1023	What is the dataset used?
1024	How do they extract topics?
1025	How does this compare to simple interpolation between a word-level and a character-level language model?
1026	What translationese effects are seen in the analysis?
1027	What languages are seen in the news and TED datasets?
1028	How are the coreference chain translations evaluated?
1029	How are the (possibly incorrect) coreference chains in the MT outputs annotated?
1030	Which three neural machine translation systems are analyzed?
1031	Which coreference phenomena are analyzed?
1032	What new interesting tasks can be solved based on the uncanny semantic structures of the embedding space?
1033	What are the uncanny semantic structures of the embedding space?
1034	What is the general framework for data exploration by semantic queries?
1035	What data exploration is supported by the analysis of these semantic structures?
1036	what are the existing models they compared with?
1037	Do they report results only on English data?
1038	What conclusions do the authors draw from their detailed analyses?
1039	Do the BERT-based embeddings improve results?
1040	What were the traditional linguistic feature-based models?
1041	What type of baseline are established for the two datasets?
1042	What model achieves state of the art performance on this task?
1043	Which multitask annotated corpus is used?
1044	What are the tasks in the multitask learning setup?
1045	What are the subtle changes in voice which have been previously overshadowed?
1046	how are rare words defined?
1047	which public datasets were used?
1048	what are the baselines?
1049	What are the results of the experiment?
1050	How was the dataset collected?
1051	What is the size of the dataset?
1052	How many different subjects does the dataset contain?
1053	How many annotators participated?
1054	How long is the dataset?
1055	Do the authors mention any possible confounds in their study?
1056	What is the relationship between the co-voting and retweeting patterns?
1057	Does the analysis find that coalitions are formed in the same way for different policy areas?
1058	What insights does the analysis give about the cohesion of political groups in the European parliament?
1059	Do they authors account for differences in usage of Twitter amongst MPs into their model?
1060	Did the authors examine if any of the MEPs used the disclaimer that retweeting does not imply endorsement on their twitter profile?
1061	How do they show their model discovers underlying syntactic structure?
1062	Which dataset do they experiment with?
1063	How do they measure performance of language model tasks?
1064	How are content clusters used to improve the prediction of incident severity?
1065	What cluster identification method is used in this paper?
1066	How can a neural model be used for a retrieval if the input is the entire Wikipedia?
1067	Which algorithm is used in the UDS-DFKI system?
1068	Does the use of out-of-domain data improve the performance of the method?
1069	Do they impose any grammatical constraints over the generated output?
1070	Why did they think this was a good idea?
1071	How many languages are included in the tweets?
1072	What languages are explored?
1073	Which countries did they look at?
1074	What QA models were used?
1075	Can this approach model n-ary relations?
1076	Was this benchmark automatically created from an existing dataset?
1077	How does morphological analysis differ from morphological inflection?
1078	What was the criterion used for selecting the lemmata?
1079	What are the architectures used for the three tasks?
1080	Which language family does Chatino belong to?
1081	What system is used as baseline?
1082	How was annotation done?
1083	How was the data collected?
1084	How do their results compare against other competitors in the PAN 2017 shared task on Author Profiling?
1085	On which task does do model do worst?
1086	On which task does do model do best?
1087	Is their implementation on CNN-DSA compared to GPU implementation in terms of power consumption, accuracy and speed?
1088	Does this implementation on CNN-DSA lead to diminishing of performance?
1089	How is Super Character method modified to handle tabular data also?
1090	How are the substitution rules built?
1091	Which dataset do they use?
1092	What baseline is used to compare the experimental results against?
1093	How does counterfactual data augmentation aim to tackle bias?
1094	In the targeted data collection approach, what type of data is targetted?
1095	Which language models do they compare against?
1096	Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?
1097	How do they determine which words are informative?
1098	What is their best performance on the largest language direction dataset?
1099	How does soft contextual data augmentation work?
1100	How does muli-agent dual learning work?
1101	Which language directions are machine translation systems of WMT evaluated on?
1102	Approximately how much computational cost is saved by using this model?
1103	What improvement does the MOE model make over the SOTA on machine translation?
1104	What improvement does the MOE model make over the SOTA on language modelling?
1105	How is the correct number of experts to use decided?
1106	What equations are used for the trainable gating network?
1107	What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?
1108	What solutions are proposed for error detection and context awareness?
1109	How is PIEWi annotated?
1110	What methods are tested in PIEWi?
1111	Which specific error correction solutions have been proposed for specialized corpora in the past?
1112	What was the criteria for human evaluation?
1113	What automatic metrics are used to measure performance of the system?
1114	What existing methods is SC-GPT compared to?
1115	Which language-pair had the better performance?
1116	Which datasets were used in the experiment?
1117	What evaluation metrics did they use?
1118	Do they evaluate only on English datasets?
1119	What are the differences in the use of emojis between gang member and the rest of the Twitter population?
1120	What are the differences in the use of YouTube links between gang member and the rest of the Twitter population?
1121	What are the differences in the use of images between gang member and the rest of the Twitter population?
1122	What are the differences in language use between gang member and the rest of the Twitter population?
1123	How is gang membership verified?
1124	Do the authors provide evidence that 'most' street gang members use Twitter to intimidate others?
1125	What is English mixed with in the TRAC dataset?
1126	Which psycholinguistic and basic linguistic features are used?
1127	How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?
1128	What are the key differences in communication styles between Twitter and Facebook?
1129	What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?
1130	What is the baseline?
1131	What datasets did they use?
1132	What scoring function does the model use to score triples?
1133	What datasets are used to evaluate the model?
1134	How long it took for each Doc2Vec model to be trained?
1135	How better are results for pmra algorithm  than Doc2Vec in human evaluation? 
1136	What Doc2Vec architectures other than PV-DBOW have been tried?
1137	What four evaluation tasks are defined to determine what influences proximity?
1138	What six parameters were optimized with grid search?
1139	What baseline models do they compare against?
1140	What are the differences with previous applications of neural networks for this task?
1141	How much improvement is gained from the proposed approaches?
1142	Is the problem of determining whether a given model would generate an infinite sequence is a decidable problem?  
1143	Is infinite-length sequence generation a result of training with maximum likelihood?
1144	What metrics are used in challenge?
1145	What model was winner of the Visual Dialog challenge 2019?
1146	What model was winner of the Visual Dialog challenge 2018?
1147	Which method for integration peforms better ensemble or consensus dropout fusion with shared parameters?
1148	How big is dataset for this challenge?
1149	What open relation extraction tasks did they experiment on?
1150	How is Logician different from traditional seq2seq models?
1151	What's the size of the previous largest OpenIE dataset?
1152	How is data for RTFM collected?
1153	How better is performance of proposed model compared to baselines?
1154	How does propose model model that capture three-way interactions?
1155	Do transferring hurt the performance is the corpora are not related?
1156	Is accuracy the only metric they used to compare systems?
1157	How do they transfer the model?
1158	Will these findings be robust through different datasets and different question answering algorithms?
1159	What is the underlying question answering algorithm?
1160	What datasets have this method been evaluated on?
1161	Is there a machine learning approach that tries to solve same problem?
1162	What DCGs are used?
1163	What else is tried to be solved other than 12 tenses, model verbs and negative form?
1164	What is used for evaluation of this approach?
1165	Is there information about performance of these conversion methods?
1166	Are there some experiments performed in the paper?
1167	How much is performance improved by disabling attention in certain heads?
1168	In which certain heads was attention disabled in experiments?
1169	What handcrafter features-of-interest are used?
1170	What subset of GLUE tasks is used?
1171	Do they predict the sentiment of the review summary?
1172	What is the performance difference of using a generated summary vs. a user-written one?
1173	Which review dataset do they use?
1174	What evaluation metrics did they look at?
1175	What datasets are used?
1176	What are the datasets used for the task?
1177	What is the accuracy of the model for the six languages tested?
1178	Which models achieve state-of-the-art performances?
1179	Is the LSTM bidirectional?
1180	What are the three languages studied in the paper?
1181	Do they use pretrained models as part of their parser?
1182	Which subtasks do they evaluate on?
1183	Do they test their approach on large-resource tasks?
1184	By how much do they, on average, outperform the baseline multilingual model on 16 low-resource tasks?
1185	How do they compute corpus-level embeddings?
1186	Which dataset do they use?
1187	Which competitive relational classification models do they test?
1188	Which tasks do they apply their method to?
1189	Which knowledge bases do they use?
1190	How do they gather human judgements for similarity between relations?
1191	Which sampling method do they use to approximate similarity between the conditional probability distributions over entity pairs?
1192	What text classification task is considered?
1193	What novel class of recurrent-like networks is proposed?
1194	Is there a formal proof that the RNNs form a representation of the group?
1195	How much bigger is Switchboard-2000 than Switchboard-300 database?
1196	How big is Switchboard-300 database?
1197	What crowdsourcing platform is used for data collection and data validation?
1198	How is validation of the data performed?
1199	Is audio data per language balanced in dataset?
1200	What is the performance of their model?
1201	Which text genres did they experiment with?
1202	What domains are detected in this paper?
1203	Why do they think this task is hard?  What is the baseline performance?
1204	Isn't simple word association enough to predict the next spell?
1205	Do they literally just treat this as "predict the next spell that appears in the text"?
1206	How well does a simple bag-of-words baseline do?
1207	Do they study frequent user responses to help automate modelling of those?
1208	How do they divide text into utterances?
1209	Do they use the same distance metric for both the SimCluster and K-means algorithm?
1210	How do they generate the synthetic dataset?
1211	how are multiple answers from multiple reformulated questions aggregated?
1212	What is the average length of the claims?
1213	What debate websites did they look at?
1214	What crowdsourcing platform did they use?
1215	Which machine baselines are used?
1216	What challenges are highlighted?
1217	What debate topics are included in the dataset?
1218	By how much, the proposed method improves BiDAF and DCN on SQuAD dataset?
1219	Do they report results only on English datasets?
1220	What are the linguistic differences between each class?
1221	What simple features are used?
1222	What lexico-syntactic cues are used to retrieve sarcastic utterances?
1223	what is the source of the song lyrics?
1224	what genre was the most difficult to classify?
1225	what word embedding techniques did they experiment with?
1226	what genres do they songs fall under?
1227	Is the filter based feature selection (FSE) a form of regularization?
1228	To what other competitive baselines is this approach compared?
1229	How is human evaluation performed, what was the criteria?
1230	How much better were results of the proposed models than base LSTM-RNN model?
1231	Which one of the four proposed models performed best?
1232	What metrics are used to measure performance of models?
1233	How much is proposed model better than baselines in performed experiments?
1234	What are state-of-the-art baselines?
1235	What two benchmark datasets are used?
1236	What languages are the model evaluated on?
1237	Do they compare to other models appart from HAN?
1238	What are the datasets used
1239	Do they evaluate only on English datasets?
1240	What evidence does visualizing the attention give to show that it helps to obtain a more robust understanding of semantics and sentiments?
1241	Which SOTA models are outperformed?
1242	What is the baseline for experiments?
1243	What is the motivation for training bi-sense embeddings?
1244	How many parameters does the model have?
1245	How many characters are accepted as input of the language model?
1246	What dataset is used for this task?
1247	What features of the document are integrated into vectors of every sentence?
1248	By how much is precission increased?
1249	Is new approach tested against state of the art?
1250	Is the dataset balanced across categories?
1251	What supervised methods are used?
1252	What labels are in the dataset?
1253	What categories does the dataset come from?
1254	What are all machine learning approaches compared in this work?
1255	Do they evaluate only on English datasets?
1256	Which patterns and rules are derived?
1257	How are customer satisfaction, customer frustration and overall problem resolution data collected?
1258	Which Twitter customer service industries are investigated?
1259	Which dialogue acts are more suited to the twitter domain?
1260	How many improvements on the French-German translation benchmark?
1261	How do they align the synthetic data?
1262	Where do they collect the synthetic data?
1263	Do they analyze what type of content Arabic bots spread in comparison to English?
1264	Do they propose a new model to better detect Arabic bots specifically?
1265	How do they prevent the model complexity increasing with the increased number of slots?
1266	What network architecture do they use for SIM?
1267	How do they measure model size?
1268	Does model uses pretrained Transformer encoders?
1269	What was previous state of the art model?
1270	What was previous state of the art accuracy on LibriSpeech benchmark?
1271	How big is LibriSpeech dataset?
1272	Which language(s) do they work with?
1273	How do they evaluate their sentence representations?
1274	Which model architecture do they for sentence encoding?
1275	How many tokens can sentences in their model at most contain?
1276	Which training objectives do they combine?
1277	Which data sources do they use?
1278	Has there been previous work on SNMT?
1279	Which languages do they experiment on?
1280	What corpora is used?
1281	Do the authors report results only on English datasets?
1282	How were breast cancer related posts compiled from the Twitter streaming API?
1283	What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?
1284	What kind of events do they extract?
1285	Is this the first paper to propose a joint model for event and temporal relation extraction?
1286	What datasets were used for this work?
1287	What languages did they experiment with?
1288	What approach performs better in experiments global latent or sequence of fine-grained latent variables?
1289	What baselines other than standard transformers are used in experiments?
1290	What three conversational datasets are used for evaluation?
1291	What previous approaches did this method outperform?
1292	How big is the Universal Dependencies corpus?
1293	What data is the Prague Dependency Treebank built on?
1294	What data is used to build the embeddings?
1295	How big is dataset used for fine-tuning model for detection of red flag medical symptoms in individual statements?
1296	Is there any explanation why some choice of language pair is better than the other?
1297	Is the new model evaluated on the tasks that BERT and ELMo are evaluated on?
1298	Does the additional training on supervised tasks hurt performance in some tasks?
1299	Which translation system do they use to translate to English?
1300	Which languages do they work with?
1301	Which pre-trained English NER model do they use?
1302	How much training data is required for each low-resource language?
1303	What are the best within-language data augmentation methods?
1304	How much of the ASR grapheme set is shared between languages?
1305	What is the performance of the model for the German sub-task A?
1306	Is the model tested for language identification?
1307	Is the model compared to a baseline model?
1308	What are the languages used to test the model?
1309	Which language has the lowest error rate reduction?
1310	What datasets did they use?
1311	Do they report results only on English data?
1312	What is the Moral Choice Machine?
1313	How is moral bias measured?
1314	What sentence embeddings were used in the previous Jentzsch paper?
1315	How do the authors define deontological ethical reasoning?
1316	How does framework automatically chooses different curricula at the evolving learning process according to the learning status of the neural dialogue generation model?
1317	What human judgement metrics are used?
1318	What automatic evaluation metrics are used?
1319	What state of the art models were used in experiments?
1320	What five dialogue attributes were analyzed?
1321	What three publicly available coropora are used?
1322	Which datasets do they use?
1323	What metrics are used for evaluation?
1324	How much training data is used?
1325	How is the training data collected?
1326	What language(s) is the model trained/tested on?
1327	was bert used?
1328	what datasets did they use?
1329	which existing metrics do they compare with?
1330	Which datasets do they evaluate on?
1331	How does their model differ from BERT?
1332	Which metrics are they evaluating with?
1333	What different properties of the posterior distribution are explored in the paper?
1334	Why does proposed term help to avoid posterior collapse?
1335	How does explicit constraint on the KL divergence term that authors propose looks like?
1336	Did they experiment with the tool?
1337	Can it be used for any language?
1338	Is this software available to the public?
1339	What shared task does this system achieve SOTA in?
1340	How are labels propagated using this approach?
1341	What information is contained in the social graph of tweet authors?
1342	What were the five English subtasks?
1343	How many CNNs and LSTMs were ensembled?
1344	what was the baseline?
1345	how many movie genres do they explore?
1346	what evaluation metrics are discussed?
1347	How big is dataset used?
1348	What is dataset used for news-driven stock movement prediction?
1349	How much better does this baseline neural model do?
1350	What is the SemEval-2016 task 8?
1351	How much faster is training time for MGNC-CNN over the baselines?
1352	What are the baseline models?
1353	By how much of MGNC-CNN out perform the baselines?
1354	What dataset/corpus is this evaluated over?
1355	What are the comparable alternative architectures?
1356	Which state-of-the-art model is surpassed by 9.68% attraction score?
1357	What is increase in percentage of humor contained in headlines generated with TitleStylist method (w.r.t. baselines)?
1358	How is attraction score measured?
1359	How is presence of three target styles detected?
1360	How is fluency automatically evaluated?
1361	What are the measures of "performance" used in this paper?
1362	What are the languages they consider in this paper?
1363	Did they experiment with tasks other than word problems in math?
1364	Do they report results only on English data?
1365	Do the authors offer any hypothesis as to why the transformations sometimes disimproved performance?
1366	What preprocessing techniques are used in the experiments?
1367	What state of the art models are used in the experiments?
1368	What evaluation metrics are used?
1369	What dataset did they use?
1370	What tasks did they experiment with?
1371	What multilingual parallel data is used for training proposed model?
1372	How much better are results of proposed model compared to pivoting method?
1373	What kind of Youtube video transcripts did they use?
1374	Which SBD systems did they compare?
1375	What makes it a more reliable metric?
1376	How much in experiments is performance improved for models trained with generated adversarial examples?
1377	How much dramatically results drop for models on generated adversarial examples?
1378	What is discriminator in this generative adversarial setup?
1379	What are benhmark datasets for paraphrase identification?
1380	What representations are presented by this paper?
1381	What corpus characteristics correlate with more equitable gender balance?
1382	What natural languages are represented in the speech resources studied?
1383	How is the delta-softmax calculated?
1384	Are some models evaluated using this metric, what are the findings?
1385	Where does proposed metric differ from juman judgement?
1386	Where does proposed metric overlap with juman judgement?
1387	Which baseline performs best?
1388	Which baselines are explored?
1389	What is the size of the corpus?
1390	How was the evaluation corpus collected?
1391	Are any machine translation sysems tried with these embeddings, what is the performance?
1392	Are any experiments performed to try this approach to word embeddings?
1393	Which two datasets does the resource come from?
1394	What model was used by the top team?
1395	What was the baseline?
1396	What is the size of the second dataset?
1397	How large is the first dataset?
1398	Who was the top-scoring team?
1399	What supervised learning tasks are attempted with these representations?
1400	What is MRR?
1401	Which techniques for word embeddings and topic models are used?
1402	Why is big data not appropriate for this task?
1403	What is an example of a computational social science NLP task?
1404	Do they report results only on English datasets?
1405	What were the previous state of the art benchmarks?
1406	How/where are the natural question generated?
1407	What is the input to the differential network?
1408	How do the authors define a differential network?
1409	How do the authors define exemplars?
1410	Is this a task other people have worked on?
1411	Where did they get the data for this project?
1412	Which major geographical regions are studied?
1413	How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]?
1414	How are the topics embedded in the #MeToo tweets extracted?
1415	How many tweets are explored in this paper?
1416	Which geographical regions correlate to the trend?
1417	How many followers did they analyze?
1418	What two components are included in their proposed framework?
1419	Which framework they propose in this paper?
1420	Why MS-MARCO is different from SQuAD?
1421	Did they experiment with pre-training schemes?
1422	What were their results on the test set?
1423	What is the size of the dataset?
1424	What was the baseline model?
1425	What models are evaluated with QAGS?
1426	Do they use crowdsourcing to collect human judgements?
1427	Which dataset(s) do they evaluate on?
1428	Which modifications do they make to well-established Seq2seq architectures?
1429	How do they measure the size of models?
1430	Do they reduce the number of parameters in their architecture compared to other direct text-to-speech models?
1431	Do they use pretrained models?
1432	What are their baseline models?
1433	How was speed measured?
1434	What were their accuracy results on the task?
1435	What is the state of the art?
1436	How was the dataset annotated?
1437	What is the size of the dataset?
1438	Where did they collect their dataset from?
1439	How much in-domain data is enough for joint models to outperform baselines?
1440	How many parameters does their proposed joint model have?
1441	How does the model work if no treebank is available?
1442	How many languages have this parser been tried on?
1443	Do they use attention?
1444	What non-annotated datasets are considered?
1445	Did they compare to Transformer based large language models?
1446	Which baselines are they using?
1447	What two types the Chinese reading comprehension dataset consists of?
1448	For which languages most of the existing MRC datasets are created?
1449	How did they induce the CFG?
1450	How big is their dataset?
1451	By how much do they outperform basic greedy and cross-entropy beam decoding?
1452	Do they provide a framework for building a sub-differentiable for any final loss metric?
1453	Do they compare partially complete sequences (created during steps of beam search) to gold/target sequences?
1454	Which loss metrics do they try in their new training procedure evaluated on the output of beam search?
1455	How are different domains weighted in WDIRL?
1456	How is DIRL evaluated?
1457	Which sentiment analysis tasks are addressed?
1458	Which NLP area have the highest average citation for woman author?
1459	Which 3 NLP areas are cited the most?
1460	Which journal and conference are cited the most in recent years?
1461	Which 5 languages appear most frequently in AA paper titles?
1462	What aspect of NLP research is examined?
1463	Are the academically younger authors cited less than older?
1464	How many papers are used in experiment?
1465	What ensemble methods are used for best model?
1466	What hyperparameters have been tuned?
1467	How much F1 was improved after adding skip connections?
1468	Where do they retrieve neighbor n-grams from in their approach?
1469	To which systems do they compare their results against?
1470	Does their combination of a non-parametric retrieval and neural network get trained end-to-end?
1471	Which similarity measure do they use in their n-gram retrieval approach?
1472	Where is MVCNN pertained?
1473	How much gain does the model achieve with pretraining MVCNN?
1474	What are the effects of extracting features of multigranular phrases?
1475	What are the effects of diverse versions of pertained word embeddings? 
1476	How is MVCNN compared to CNN?
1477	What is the highest accuracy score achieved?
1478	What is the size range of the datasets?
1479	Does the paper report F1-scores for the age and language variety tasks?
1480	Are the models compared to some baseline models?
1481	What are the in-house data employed?
1482	What are the three datasets used in the paper?
1483	What are the potentials risks of this approach?
1484	What elements of natural language processing are proposed to analyze qualitative data?
1485	How does the method measure the impact of the event on market prices?
1486	How is sentiment polarity measured?
1487	Which part of the joke is more important in humor?
1488	What is improvement in accuracy for short Jokes in relation other types of jokes?
1489	What kind of humor they have evaluated?
1490	How they evaluate if joke is humorous or not?
1491	Do they report results only on English data?
1492	Do the authors have a hypothesis as to why morphological agreement is hardly learned by any model?
1493	Which models are best for learning long-distance movement?
1494	Where does the data in CoLA come from?
1495	How is the CoLA grammatically annotated?
1496	What baseline did they compare Entity-GCN to?
1497	How many documents at a time can Entity-GCN handle?
1498	Did they use a relation extraction method to construct the edges in the graph?
1499	How did they get relations between mentions?
1500	How did they detect entity mentions?
1501	What is the metric used with WIKIHOP?
1502	What performance does the Entity-GCN get on WIKIHOP?
1503	Do they evaluate only on English datasets?
1504	What baseline models are used?
1505	What classical machine learning algorithms are used?
1506	What are the different methods used for different corpora?
1507	In which domains is sarcasm conveyed in different ways?
1508	What modalities are being used in different datasets?
1509	What is the difference between Long-short Term Hybrid Memory and LSTMs?
1510	Do they report results only on English data?
1511	What provisional explanation do the authors give for the impact of document context?
1512	What document context was added?
1513	What were the results of the first experiment?
1514	How big is the evaluated dataset?
1515	By how much does their model outperform existing methods?
1516	What is the performance of their model?
1517	What are the existing methods mentioned in the paper?
1518	Does having constrained neural units imply word meanings are fixed across different context?
1519	Do they perform a quantitative analysis of their model displaying knowledge distortions?
1520	How do they damage different neural modules?
1521	Which weights from their model do they analyze?
1522	Do all the instances contain code-switching?
1523	What embeddings do they use?
1524	Do they perform some annotation?
1525	Do they use dropout?
1526	What definition of hate speech do they use?
1527	What are the other models they compare to?
1528	What is the agreement value for each dataset?
1529	How many annotators participated?
1530	How long are the datasets?
1531	What are the sources of the data?
1532	What is the new labeling strategy?
1533	Which future direction in NLG are discussed?
1534	What experimental phenomena are presented?
1535	How strategy-based methods handle obstacles in NLG?
1536	How architecture-based method handle obstacles in NLG?
1537	How are their changes evaluated?
1538	What baseline is used for the verb classification experiments?
1539	What clustering algorithm is used on top of the VerbNet-specialized representations?
1540	How many words are translated between the cross-lingual translation pairs?
1541	What are the six target languages?
1542	what classifiers were used in this paper?
1543	what are their evaluation metrics?
1544	what types of features were used?
1545	what lexical features did they experiment with?
1546	what is the size of the dataset?
1547	what datasets were used?
1548	what are the three reasons everybody hates them?
1549	How are seed dictionaries obtained by fully unsupervised methods?
1550	How does BLI measure alignment quality?
1551	What methods were used for unsupervised CLWE?
1552	What is the size of the released dataset?
1553	Were the OpenIE systems more accurate on some scientific disciplines than others?
1554	What is the most common error type?
1555	Which OpenIE systems were used?
1556	What is the role of crowd-sourcing?
1557	How are meta vertices computed?
1558	How are graphs derived from a given text?
1559	In what sense if the proposed method interpretable?
1560	how are the bidirectional lms obtained?
1561	what metrics are used in evaluation?
1562	what results do they achieve?
1563	what previous systems were compared to?
1564	what are the evaluation datasets?
1565	Are datasets publicly available?
1566	Are this models usually semi/supervised or unsupervised?
1567	Is there any concrete example in the paper that shows that this approach had huge impact on drug discovery?
1568	Do the authors analyze what kinds of cases their new embeddings fail in where the original, less-interpretable embeddings didn't?
1569	When they say "comparable performance", how much of a performance drop do these new embeddings result in?
1570	How do they evaluate interpretability?
1571	What types of word representations are they evaluating?
1572	What type of recurrent layers does the model use?
1573	What is a word confusion network?
1574	What type of simulations of real-time data feeds are used for validaton?
1575	How are FHIR and RDF combined?
1576	What are the differences between FHIR and RDF?
1577	What do FHIR and RDF stand for?
1578	What is the motivation behind the work? Why question generation is an important task?
1579	Why did they choose WER as evaluation metric?
1580	What evaluation metrics were used in the experiment?
1581	What kind of instructional videos are in the dataset?
1582	What baseline algorithms were presented?
1583	What is the source of the triples?
1584	How much better is performance of the proposed model compared to the state of the art in these various experiments?
1585	What was state of the art on SemEval-2014 task4 Restaurant and Laptop dataset?
1586	What was previous state-of-the-art on four Chinese reviews datasets?
1587	In what four Chinese review datasets does LCF-ATEPC achieves state of the art?
1588	Why authors think that researches do not pay attention to the research of the Chinese-oriented ABSA task?
1589	What is specific to Chinese-oriented ABSA task, how is it different from other languages?
1590	what is the size of this dataset?
1591	did they use a crowdsourcing platform for annotations?
1592	where does the data come from?
1593	What is the criteria for a good metric?
1594	What are the three limitations?
1595	What is current state-of-the-art model?
1596	Which language(s) are found in the WSD datasets?
1597	What datasets are used for testing?
1598	How does TP-N2F compare to LSTM-based Seq2Seq in terms of training and inference speed?
1599	What is the performance proposed model achieved on AlgoList benchmark?
1600	What is the performance proposed model achieved on MathQA?
1601	How do previous methods perform on the Switchboard Dialogue Act and DailyDialog datasets?
1602	What previous methods is the proposed method compared against?
1603	What is dialogue act recognition?
1604	Which natural language(s) are studied?
1605	Does the performance necessarily drop when more control is desired?
1606	How does the model perform in comparison to end-to-end headline generation models?
1607	How is the model trained to do only content selection?
1608	What is the baseline model used?
1609	Is this auto translation tool based on neural networks?
1610	What are results of public code repository study?
1611	Where is the dataset from?
1612	What data augmentation techniques are used?
1613	Do all teams use neural networks for their models?
1614	How are the models evaluated?
1615	What is the baseline model?
1616	What domains are present in the data?
1617	How many texts/datapoints are in the SemEval, TASS and SENTIPOLC datasets?
1618	In which languages did the approach outperform the reported results?
1619	What eight language are reported on?
1620	What are the components of the multilingual framework?
1621	Is the proposed method compared to previous methods?
1622	What metrics are used to evaluate results?
1623	Which is the baseline model?
1624	How big is the Babel database?
1625	What is the main contribution of the paper? 
1626	What training settings did they try?
1627	How do they get the formal languages?
1628	Are the unobserved samples from the same distribution as the training data?
1629	By how much does their model outperform the baseline in the cross-domain evaluation?
1630	What are the performance results?
1631	What is a confusion network or lattice?
1632	What dataset is used for training?
1633	How close do clusters match to ground truth tone categories?
1634	what are the evaluation metrics?
1635	which datasets were used in evaluation?
1636	what are the baselines?
1637	What multilingual word representations are used?
1638	Do the authors identify any cultural differences in irony use?
1639	What neural architectures are used?
1640	What text-based features are used?
1641	What monolingual word representations are used?
1642	Does the proposed method outperform a baseline?
1643	What type of RNN is used?
1644	What do they constrain using integer linear programming?
1645	Do they build one model per topic or on all topics?
1646	Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?
1647	Do they evaluate their framework on content of low lexical variety?
1648	Do the authors report on English datasets only?
1649	Which supervised learning algorithms are used in the experiments?
1650	How in YouTube content translated into a vector format?
1651	How is the ground truth of gang membership established in this dataset?
1652	Do they evaluate ablated versions of their CNN+RNN model?
1653	Do they single out a validation set from the fixed SRE training set?
1654	How well does their system perform on the development set of SRE?
1655	Which are the novel languages on which SRE placed emphasis on?
1656	Does this approach perform better than context-based word embeddings?
1657	Have the authors tried this approach on other languages?
1658	What features did they train on?
1659	How big is the test set?
1660	What is coattention?
1661	What off-the-shelf QA model was used to answer sub-questions?
1662	How large is the improvement over the baseline?
1663	What is the strong baseline that this work outperforms?
1664	Which dataset do they use?
1665	Do they trim the search space of possible output sequences?
1666	Which model architecture do they use to build a model?
1667	Do they compare simultaneous translation performance to regular machine translation?
1668	Which metrics do they use to evaluate simultaneous translation?
1669	How big are FigureQA and DVQA datasets?
1670	What models other than SAN-VOES are trained on new PlotQA dataset?
1671	Do the authors report only on English language data?
1672	What dataset of tweets is used?
1673	What external sources of information are used?
1674	What linguistic features are used?
1675	What are the key issues around whether the gold standard data produced in such an annotation is reliable? 
1676	How were the machine learning papers from ArXiv sampled?
1677	What are the core best practices of structured content analysis?
1678	In what sense is data annotation similar to structured content analysis? 
1679	What additional information is found in the dataset?
1680	Is the dataset focused on a region?
1681	Over what period of time were the tweets collected?
1682	Are the tweets location-specific?
1683	How big is the dataset?
1684	Do the authors suggest any future extensions to this work?
1685	Which of the classifiers showed the best performance?
1686	Were any other word similar metrics, besides Jaccard metric, tested?
1687	How are the keywords associated with events such as protests selected?
1688	How many speeches are in the dataset?
1689	What classification models were used?
1690	Do any speeches not fall in these categories?
1691	What is different in BERT-gen from standard BERT?
1692	How are multimodal representations combined?
1693	What is the problem with existing metrics that they are trying to address?
1694	How do they evaluate their proposed metric?
1695	What is a facet?
1696	How are discourse embeddings analyzed?
1697	What was the previous state-of-the-art?
1698	How are discourse features incorporated into the model?
1699	What discourse features are used?
1700	How are proof scores calculated?
1701	What are proof paths?
1702	What is the size of the model?
1703	What external sources are used?
1704	What submodules does the model consist of?
1705	How they add human prefference annotation to fine-tuning process?
1706	What previous automated evalution approaches authors mention?
1707	How much better peformance is achieved in human evaluation when model is trained considering proposed metric?
1708	Do the authors suggest that proposed metric replace human evaluation on this task?
1709	What is the training objective of their pair-to-sequence model?
1710	How do they ensure the generated questions are unanswerable?
1711	Does their approach require a dataset of unanswerable questions mapped to similar answerable questions?
1712	What conclusions are drawn from these experiments?
1713	What experiments are presented?
1714	What is specific about the specific embeddings?
1715	What embedding algorithm is used to build the embeddings?
1716	How was the KGR10 corpus created?
1717	How big are improvements with multilingual ASR training vs single language training?
1718	How much transcribed data is available for for Ainu language?
1719	What is the difference between speaker-open and speaker-closed setting?
1720	What baseline approaches do they compare against?
1721	How do they train the retrieval modules?
1722	How do they model the neural retrieval modules?
1723	Retrieval at what level performs better, sentence level or paragraph level?
1724	How much better performance of proposed model compared to answer-selection models?
1725	How are some nodes initially connected based on text structure?
1726	how many domains did they experiment with?
1727	what pretrained models were used?
1728	What domains are contained in the polarity classification dataset?
1729	How long is the dataset?
1730	What machine learning algorithms are used?
1731	What is a string kernel?
1732	Which dataset do they use to learn embeddings?
1733	How do they correlate NED with emotional bond levels?
1734	What was their F1 score on the Bengali NER corpus?
1735	Which languages are evaluated?
1736	Which model have the smallest Character Error Rate and which have the smallest Word Error Rate?
1737	What will be in focus for future work?
1738	Which acoustic units are more suited to model the French language?
1739	What are the existing end-to-end ASR approaches for the French language?
1740	How much is decoding speed increased by increasing encoder and decreasing decoder depth?
1741	Did they experiment on this dataset?
1742	What language are the conversations in?
1743	How did they annotate the dataset?
1744	What annotations are in the dataset?
1745	What is the size of the dataset?
1746	What language platform does the data come from?
1747	Which two schemes are used?
1748	How many examples do they have in the target domain?
1749	Does Grail accept Prolog inputs?
1750	What formalism does Grail use?
1751	Which components of QA and QG models are shared during training?
1752	How much improvement does jointly learning QA and QG give, compared to only training QA?
1753	Do they test their word embeddings on downstream tasks?
1754	What are the main disadvantages of their proposed word embeddings?
1755	What dimensions of word embeddings do they produce using factorization?
1756	On which dataset(s) do they compute their word embeddings?
1757	Do they measure computation time of their factorizations compared to other word embeddings?
1758	What datasets are experimented with?
1759	What is the baseline model?
1760	What model do they train?
1761	What are the eight features mentioned?
1762	How many languages are considered in the experiments?
1763	How did they evaluate the system?
1764	Where did they get training data?
1765	What extraction model did they use?
1766	Which datasets did they experiment on?
1767	What types of facts can be extracted from QA pairs that can't be extracted from general text?
1768	How do slot binary classifiers improve performance?
1769	What baselines have been used in this work?
1770	Do sluice networks outperform non-transfer learning approaches?
1771	What is hard parameter sharing?
1772	How successful are they at matching names of authors in Japanese and English?
1773	Is their approach applicable to papers outside computer science?
1774	Do they translate metadata from Japanese papers to English?
1775	what bottlenecks were identified?
1776	What is grounded language understanding?
1777	Does the paper report the performance on the task of a Neural Machine Translation model?
1778	What are the predefined morpho-syntactic patterns used to filter the training data?
1779	Is the RNN model evaluated against any baseline?
1780	Which languages are used in the paper?
1781	What metrics are used for evaluation?
1782	What baselines are used?
1783	Which model is used to capture the implicit structure?
1784	How is the robustness of the model evaluated?
1785	How is the effectiveness of the model evaluated?
1786	Do they assume sentence-level supervision?
1787	By how much do they outperform BiLSTMs in Sentiment Analysis?
1788	Does their model have more parameters than other models?
1789	what state of the accuracy did they obtain?
1790	what models did they compare to?
1791	which benchmark tasks did they experiment on?
1792	Are recurrent neural networks trained on perturbed data?
1793	How does their perturbation algorihm work?
1794	Which language is divided into six dialects in the task mentioned in the paper?
1795	What is one of the first writing systems in the world?
1796	How do they obtain distant supervision rules for predicting relations?
1797	Which structured prediction approach do they adopt for temporal entity extraction?
1798	Which evaluation metric has been measured?
1799	What is the WordNet counterpart for Persian?
1800	Do they consider relations other than binary relations?
1801	Are the grammar clauses manually created?
1802	Do they use an NER system in their pipeline?
1803	What large corpus is used for experiments?
1804	Are any of the utterances ungrammatical?
1805	How is the proficiency score calculated?
1806	What proficiency indicators are used to the score the utterances?
1807	What accuracy is achieved by the speech recognition system?
1808	How is the speech recognition system evaluated?
1809	How many of the utterances are transcribed?
1810	How many utterances are in the corpus?
1811	By how much does their model outperform both the state-of-the-art systems?
1812	What is the state-of-the art?
1813	How do they identify abbreviations?
1814	What kind of model do they build to expand abbreviations?
1815	Do they use any knowledge base to expand abbreviations?
1816	In their used dataset, do they study how many abbreviations are ambiguous?
1817	Which dataset do they use to build their model?
1818	What is the domain of their collected corpus?
1819	What was the performance on the self-collected corpus?
1820	What is the size of their dataset?
1821	What is the source of the CAIS dataset?
1822	What were the baselines models?
1823	Are the document vectors that the authors introduce evaluated in any way other than the new way the authors propose?
1824	According to the authors, why does the CNN model exhibit a higher level of explainability?
1825	Does the LRP method work in settings that contextualize the words with respect to one another?
1826	How do they incorporate lexicon into the neural network?
1827	What is the source of their lexicon?
1828	What was their performance?
1829	How long is the dataset used for training?
1830	What embeddings do they use?
1831	did they use other pretrained language models besides bert?
1832	how was the dataset built?
1833	what is the size of BoolQ dataset?
1834	what processing was done on the speeches before being parsed?
1835	What programming language is the tool written in?
1836	What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?
1837	Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?
1838	What kind is noise is present in typical industrial data?
1839	What is the reason behind the drop in performance using BERT for some popular task?
1840	How they observe that fine-tuning BERT on a specific task does not improve its prunability?
1841	How much is pre-training loss increased in Low/Medium/Hard level of pruning?
1842	How do they gather human reviews?
1843	Do they explain model predictions solely on attention weights?
1844	Can their method of creating more informative visuals be applied to tasks other than turn taking in conversations?
1845	What was the baseline?
1846	What is the average length of the recordings?
1847	How big was the dataset presented?
1848	What were their results?
1849	Does a neural scoring function take both the question and the logical form as inputs?
1850	What is the source of the paraphrases of the questions?
1851	Does the dataset they use differ from the one used by Pasupat and Liang, 2015?
1852	Did they experiment on this corpus?
1853	Is the model compared against a linear regression baseline?
1854	What is the prediction accuracy of the model?
1855	What is the dataset used in the paper?
1856	How does the differential privacy mechanism work?
1857	How does the SCAN dataset evaluate compositional generalization?
1858	Is their model fine-tuned also on all available data, what are results?
1859	How much does this system outperform prior work?
1860	What are the baseline systems that are compared against?
1861	What standard large speaker verification corpora is used for evaluation?
1862	What systems are tested?
1863	How many examples are there in the source domain?
1864	How many examples are there in the target domain?
1865	Did they only experiment with captioning task?
1866	how well this method is compared to other method?
1867	What benchmark datasets they use?
1868	what is the proposed Progressive Dynamic Hurdles method?
1869	What is in the model search space?
1870	How much energy did the NAS consume?
1871	How does Progressive Dynamic Hurdles work?
1872	Do they beat current state-of-the-art on SICK?
1873	How do they combine MonaLog with BERT?
1874	How do they select monotonicity facts?
1875	How does the model recognize entities and their relation to answers at inference time when answers are not accessible?
1876	What other solutions do they compare to?
1877	How does the gatint mechanism combine word and character information?
1878	Which dataset do they use?
1879	How is the PBMT system trained?
1880	Which NMT architecture do they use?
1881	Do they train the NMT model on PBMT outputs?
1882	Is the corpus annotated?
1883	How is the corpus normalized?
1884	What are the 12 categories devised?
1885	Is the corpus annotated with a phonetic transcription?
1886	Is the corpus annotated with Part-of-Speech tags?
1887	what evaluation methods are discussed?
1888	what are the off-the-shelf systems discussed in the paper?
1889	How is "hirability" defined?
1890	Have the candidates given their consent to have their videos used for the research?
1891	Do they analyze if their system has any bias?
1892	Is there any ethical consideration in the research?
1893	What low-resource languages were used in this work?
1894	What classification task was used to evaluate the cross-lingual adaptation method described in this work?
1895	How many parameters does the presented model have?
1896	How do they measure the quality of detection?
1897	What previous approaches are considered?
1898	How is the back-translation model trained?
1899	Are the rules dataset specific?
1900	How many rules had to be defined?
1901	What datasets are used in this paper?
1902	How much labeled data is available for these two languages?
1903	What was performance of classifiers before/after using distant supervision?
1904	What classifiers were used in experiments?
1905	In which countries are Hausa and Yor\`ub\'a spoken?
1906	What is the agreement score of their annotated dataset?
1907	What is the size of the labelled dataset?
1908	Which features do they use to model Twitter messages?
1909	Do they allow for messages with vaccination-related key terms to be of neutral stance?
1910	How big are the datasets used?
1911	Is this a span-based (extractive) QA task?
1912	Are the contexts in a language different from the questions?
1913	What datasets are used for training/testing models? 
1914	How better is gCAS approach compared to other approaches?
1915	What is specific to gCAS cell?
1916	What dataset do they evaluate their model on?
1917	What is the source of external knowledge?
1918	Which of their proposed attention methods works better overall?
1919	Which dataset of texts do they use?
1920	Do they measure how well they perform on longer sequences specifically?
1921	Which other embeddings do they compare against?
1922	What were the sizes of the test sets?
1923	What training data did they use?
1924	What domains do they experiment with?
1925	What games are used to test author's methods?
1926	How is the domain knowledge transfer represented as knowledge graph?
1927	What was the baseline?
1928	Which datasets are used?
1929	Which six languages are experimented with?
1930	What shallow local features are extracted?
1931	Do they compare results against state-of-the-art language models?
1932	Do they integrate the second-order term in the mLSTM?
1933	Which dataset do they train their models on?
1934	How much does it minimally cost to fine-tune some model according to benchmarking framework?
1935	What models are included in baseline benchmarking results?
1936	did they compare with other evaluation metrics?
1937	which datasets were used in validation?
1938	It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?
1939	What multimodal representations are used in the experiments?
1940	How much better is inference that has addition of image representation compared to text-only representations? 
1941	How they compute similarity between the representations?
1942	How big is vSTS training data?
1943	Which evaluation metrics do they use for language modelling?
1944	Do they do quantitative quality analysis of learned embeddings?
1945	Do they evaluate on downstream tasks?
1946	Which corpus do they use?
1947	How much more accurate is the model than the baseline?
1948	How big is improvement over the old  state-of-the-art performance on CoNLL-2009 dataset?
1949	What is new state-of-the-art performance on CoNLL-2009 dataset?
1950	How big is CoNLL-2009 dataset?
1951	What different approaches of encoding syntactic information authors present?
1952	What are two strong baseline methods authors refer to?
1953	How many category tags are considered?
1954	What domain does the dataset fall into?
1955	What ASR system do they use?
1956	What is the state of the art?
1957	How big are datasets used in experiments?
1958	What previously annotated databases are available?
1959	Do they address abstract meanings and concepts separately?
1960	Do they argue that all words can be derived from other (elementary) words?
1961	Do they break down word meanings into elementary particles as in the standard model of quantum theory?
1962	How big is the dataset used?
1963	How they prove that multi-head self-attention is at least as powerful as convolution layer? 
1964	Is there a way of converting existing convolution layers into self-attention to perform very same convolution?
1965	What authors mean by sufficient number of heads?
1966	Is there any nonnumerical experiment that also support author's claim, like analysis of attention layers in publicly available networks? 
1967	What numerical experiments they perform?
1968	What dataset is used?
1969	What language do they look at?
1970	What dierse domains and languages are present in new datasets?
1971	Are their corpus and software public?
1972	How are EAC evaluated?
1973	What are the currently available datasets for EAC?
1974	What are the research questions posed in the paper regarding EAC studies?
1975	What evaluation metrics did they use?
1976	What is triangulation?
1977	What languages are explored in this paper?
1978	Did they experiment with the dataset on some tasks?
1979	How better does the hybrid tiled CNN model perform than its counterparts?
1980	Do they use pretrained word embeddings?
1981	Do they use skipgram version of word2vec?
1982	What domains are considered that have such large vocabularies?
1983	Do they perform any morphological tokenization?
1984	How many nodes does the cluster have?
1985	What data do they train the language models on?
1986	Do they report BLEU scores?
1987	What languages do they use?
1988	What architectures are explored to improve the seq2seq model?
1989	Why is this work different from text-only UNMT?
1990	What is baseline used?
1991	Did they evaluate against baseline?
1992	How they evaluate their approach?
1993	How large is the corpus?
1994	Which document classifiers do they experiment with?
1995	How large is the dataset?
1996	What evaluation metrics are used to measure diversity?
1997	How is some information lost in the RNN-based generation models?
1998	What is the model accuracy?
1999	How do the authors define fake news?
2000	What dataset is used?
2001	Did they use other evaluation metrics?
2002	What was their perplexity score?
2003	What languages are explored in this paper?
2004	What parallel corpus did they use?
2005	What datasets are used for experiments on three other tasks?
2006	In which setting they achieve the state of the art?
2007	What accuracy do they approach with their proposed method?
2008	What they formulate the question generation as?
2009	What two main modules their approach consists of?
2010	Are there syntax-agnostic SRL models before?
2011	What is the biaffine scorer?
2012	What languages are were included in the dataset of hateful content?
2013	How was reliability measured?
2014	How did the authors demonstrate that showing a hate speech definition caused annotators to partially align their own opinion with the definition?
2015	What definition was one of the groups was shown?
2016	Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?
2017	How were potentially hateful messages identified?
2018	Which embeddings do they detect biases in?
2019	What does their system consist of?
2020	What are the two PharmaCoNER subtasks?
2021	What neural language models are explored?
2022	How do they perform data augmentation?
2023	What proportion of negative-examples do they use?
2024	Do the authors mention any possible confounds in their study?
2025	What are the characteristics of the city dialect?
2026	What are the characteristics of the rural dialect?
2027	What are their baseline methods?
2028	Which datasets are used for evaluation?
2029	Which of the model yields the best performance?
2030	What is the performance of the models on the tasks?
2031	How is the data automatically generated?
2032	Do they fine-tune the used word embeddings on their medical texts?
2033	Which word embeddings do they use to represent medical visits?
2034	Do they explore similarity of texts across different doctors?
2035	Which clustering technique do they use on partients' visits texts?
2036	What is proof that proposed functional form approximates well generalization error in practice?
2037	How is proposed functional form constructed for some model?
2038	What other non-neural baselines do the authors compare to? 
2039	How much better is performance of proposed approach compared to greedy decoding baseline?
2040	What environment is used for self-critical sequence training?
2041	What baseline function is used in REINFORCE algorithm?
2042	What baseline model is used for comparison?
2043	Is Aristo just some modern NLP model (ex. BERT) finetuned od data specific for this task?
2044	On what dataset is Aristo system trained?
2045	Does they focus on any specific product/service domain?
2046	What are the baselines?
2047	How is fluency of generated text evaluated?
2048	How is faithfulness of the resulting text evaluated?
2049	How are typing hints suggested?
2050	What is the effectiveness plan generation?
2051	How is neural planning component trained?
2052	How do they evaluate interpretability in this paper?
2053	How much better performing is the proposed method over the baselines?
2054	What baselines are the proposed method compared against?
2055	What dataset/corpus is this work evaluated over?
2056	How many roles are proposed?
2057	What language technologies have been introduced in the past?
2058	Does the dataset contain non-English reviews?
2059	Does the paper report the performance of the method when is trained for more than 8 epochs?
2060	What do the correlation demonstrate? 
2061	On Twitter, do the demographic attributes and answers show more correlations than on Yahoo! Answers?
2062	How many demographic attributes they try to predict?
2063	What evaluation metrics do they use?
2064	How do they define local variance?
2065	How do they quantify alignment between the embeddings of a document and its translation?
2066	Does adversarial learning have stronger performance gains for text classification, or for NER?
2067	Do any of the evaluations show that adversarial learning improves performance in at least two different language families?
2068	what experiments are conducted?
2069	what opportunities are highlighted?
2070	how do they measure discussion quality?
2071	do they use a crowdsourcing platform?
2072	what were the baselines?
2073	Which soft-selection approaches are evaluated?
2074	Is the model evaluated against the baseline also on single-aspect sentences?
2075	Is the accuracy of the opinion snippet detection subtask reported?
2076	What were the baselines?
2077	What metric was used in the evaluation step?
2078	What did they pretrain the model on?
2079	What does the data cleaning and filtering process consist of?
2080	What unlabeled corpus did they use?
2081	How efective is MCDN for ambiguous and implicit causality inference compared to state-of-the-art?
2082	What performance did proposed method achieve, how much better is than previous state-of-the-art?
2083	What was previous state-of-the-art approach?
2084	How is Relation network used to infer causality at segment level?
2085	What is the TREC-CAR dataset?
2086	How does their model utilize contextual information for each work in the given sentence in a multi-task setting? setting?
2087	What metris are used for evaluation?
2088	How better is proposed model compared to baselines?
2089	What are the baselines?
2090	How big is slot filing dataset?
2091	Which machine learning models do they use to correct run-on sentences?
2092	How large is the dataset they generate?
2093	What are least important components identified in the the training of VQA models?
2094	What type of experiments are performed?
2095	What components are identified as core components for training VQA models?
2096	what approaches are compared?
2097	What model do they use a baseline to estimate satisfaction?
2098	what semantically conditioned models did they compare with?
2099	Do they differentiate insights where they are dealing with learned or engineered representations?
2100	Do they show an example of usage for INFODENS?
2101	What kind of representation exploration does INFODENS provide?
2102	What models do they compare to?
2103	What is the optimal trading strategy based on reinforcement learning?
2104	Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?
2105	Which tweets are used to output the daily sentiment signal?
2106	What is the baseline machine learning prediction approach?
2107	What are the weaknesses of their proposed interpretability quantification method?
2108	What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?
2109	How do they generate a graphic representation of a query from a query?
2110	How do they gather data for the query explanation problem?
2111	Which query explanation method was preffered by the users in terms of correctness?
2112	Do they conduct a user study where they show an NL interface with and without their explanation?
2113	How do the users in the user studies evaluate reliability of a NL interface?
2114	What was the task given to workers?
2115	How was lexical diversity measured?
2116	How many responses did they obtain?
2117	What crowdsourcing platform was used?
2118	Are results reported only for English data?
2119	Which existing models does this approach outperform?
2120	What human evaluation method is proposed?
2121	How is human evaluation performed, what were the criteria?
2122	What automatic metrics are used?
2123	What other kinds of generation models are used in experiments?
2124	How does discrete latent variable has an explicit semantic meaning to improve the CVAE on short-text conversation?
2125	What news dataset was used?
2126	How do they determine similarity between predicted word and topics?
2127	What is the language model pre-trained on?
2128	What languages are represented in the dataset?
2129	Which existing language ID systems are tested?
2130	How was the one year worth of data collected?
2131	Which language family does Mboshi belong to?
2132	Does the paper report any alignment-only baseline?
2133	What is the dataset used in the paper?
2134	How is the word segmentation task evaluated?
2135	What are performance compared to former models?
2136	How faster is training and decoding compared to former models?
2137	What datasets was the method evaluated on?
2138	Is the model evaluated against a baseline?
2139	How many people are employed for the subjective evaluation?
2140	What other embedding models are tested?
2141	How is performance measured?
2142	How are rare words defined?
2143	What datasets are used to evaluate the model?
2144	What other datasets are used?
2145	What is the size of the dataset?
2146	What is the source of the dataset?
2147	What were the baselines?
2148	How do they show that acquiring names of places helps self-localization?
2149	How do they evaluate how their model acquired words?
2150	Which method do they use for word segmentation?
2151	Does their model start with any prior knowledge of words?
2152	What were the baselines?
2153	What metadata is included?
2154	How many expert journalists were there?
2155	Do the images have multilingual annotations or monolingual ones?
2156	Could you learn such embedding simply from the image annotations and without using visual information?
2157	How much important is the visual grounding in the learning of the multilingual representations?
2158	How is the generative model evaluated?
2159	How do they evaluate their method?
2160	What is an example of a health-related tweet?
2161	Was the introduced LSTM+CNN model trained on annotated data in a supervised fashion?
2162	What is the challenge for other language except English
2163	How many categories of offensive language were there?
2164	How large was the dataset of Danish comments?
2165	Who were the annotators?
2166	Is is known whether Sina Weibo posts are censored by humans or some automatic classifier?
2167	Which matching features do they employ?
2168	How often are the newspaper websites crawled daily?
2169	How much better in terms of JSD measure did their model perform?
2170	What does the Jensen-Shannon distance measure?
2171	Which countries and languages do the political speeches and manifestos come from?
2172	Do changes in policies of the political actors account for all of the mistakes the model made?
2173	What model are the text features used in to provide predictions?
2174	By how much does their method outperform the multi-head attention model?
2175	How large is the corpus they use?
2176	Does each attention head in the decoder calculate the same output?
2177	Which distributional methods did they consider?
2178	Which benchmark datasets are used?
2179	What hypernymy tasks do they study?
2180	Do they repot results only on English data?
2181	What were the variables in the ablation study?
2182	How many shared layers are in the system?
2183	How many additional task-specific layers are introduced?
2184	What is barycentric Newton diagram?
2185	Do they propose any solution to debias the embeddings?
2186	How are these biases found?
2187	How many layers of self-attention does the model have?
2188	Is human evaluation performed?
2189	What are the three datasets used?
2190	Did they experiment with the corpus?
2191	How were the feature representations evaluated?
2192	What linguistic features were probed for?
2193	Does the paper describe experiments with real humans?
2194	What are bottleneck features?
2195	What languages are considered?
2196	Do they compare speed performance of their model compared to the ones using the LID model?
2197	How do they obtain language identities?
2198	What other multimodal knowledge base embedding methods are there?
2199	What is the data selection paper in machine translation
2200	Do they compare computational time of AM-softmax versus Softmax?
2201	Do they visualize the difference between AM-Softmax and regular softmax?
2202	what metrics were used for evaluation?
2203	what are the state of the art methods?
2204	What datasets do they use for the tasks?
2205	What evaluation metrics do they use?
2206	What performance is achieved?
2207	Do they use BERT?
2208	What is their baseline?
2209	Which two datasets is the system tested on?
2210	Which four languages do they experiment with?
2211	Does DCA or GMM-based attention perform better in experiments?
2212	How they compare varioius mechanisms in terms of naturalness?
2213	What evaluation metric is used?
2214	What datasets are used?
2215	Is the origin of the dialogues in corpus some video game and what game is that?
2216	Is any data-to-text generation model trained on this new corpus, what are the results?
2217	How the authors made sure that corpus is clean despite being crowdsourced?
2218	Do they build a generative probabilistic language model for sign language?
2219	Does CLSTM have any benefits over BERT?
2220	How do they obtain human generated policies?
2221	How many agents do they ensemble over?
2222	What is the task of slot filling?
2223	Do they report results only on English data?
2224	Does this system improve on the SOTA?
2225	How are the potentially relevant text fragments identified?
2226	What algorithm and embedding dimensions are used to build the task-specific embeddings?
2227	What data is used to build the task-specific embeddings?
2228	Do they evaluate the syntactic parses?
2229	What knowledge bases do they use?
2230	Which dataset do they use?
2231	What pre-trained models did they compare to?
2232	How does the fusion method work?
2233	What dataset did they use?
2234	What benchmarks did they experiment on?
2235	What were the evaluation metrics used?
2236	What is the size of the dataset?
2237	What multi-domain dataset is used?
2238	Which domains did they explored?
2239	Do they report results only on English data?
2240	Which is the best performing method?
2241	What size are the corpora?
2242	What is a self-compiled corpus?
2243	What are the 12 AV approaches which are examined?
2244	how was annotation done?
2245	what is the source of the new dataset?
2246	Do the authors give examples of positive and negative sentiment with regard to the virus?
2247	Which word frequencies reflect on the psychology of the twitter users, according to the authors?
2248	Do they specify which countries they collected twitter data from?
2249	Do they collect only English data?
2250	How do they measure correlation between the prediction and explanation quality?
2251	Does the Agent ask for a value of a variable using natural language generated text?
2252	What models does this overview cover?
2253	What datasets are used to evaluate the introduced method?
2254	What are the results achieved from the introduced method?
2255	How do they incorporate human advice?
2256	What do they learn jointly?
2257	Is this an English-language dataset?
2258	What affective-based features are used?
2259	What conversation-based features are used?
2260	What are the evaluation metrics used?
2261	Do they report results only on English datasets?
2262	What datasets or tasks do they conduct experiments on?
2263	How big is performance improvement proposed methods are used?
2264	How authors create adversarial test set to measure model robustness?
2265	Do they compare with the MAML algorithm?
2266	By how much does transfer learning improve performance on this task?
2267	What baseline is used?
2268	What topic clusters are identified by LDA?
2269	What are the near-offensive language categories?
2270	How much do they outperform previous state-of-the-art?
2271	How do they generate the auxiliary sentence?
2272	Have any baseline model been trained on this abusive language dataset?
2273	How big are this dataset and catalogue?
2274	What is open website for cataloguing abusive language data?
2275	how many speeches are in the dataset?
2276	How big is the provided treebank?
2277	What is LAS metric?
2278	is the dataset balanced across the four languages?
2279	what evaluation metrics were used?
2280	what dataset was used?
2281	What are the citation intent labels in the datasets?
2282	What is the size of ACL-ARC datasets?
2283	Is the affect of a word affected by context?
2284	asdfasdaf
2285	asdfasdf
2286	asdfasd
2287	asdf
2288	How is quality of annotation measured?
2289	On what data is the model evaluated?
2290	What accuracy score do they obtain?
2291	What is their baseline model?
2292	What is the size of the dataset?
2293	What is the 12 class bilingual text?
2294	Which languages do they focus on?
2295	Which are the sequence model architectures this method can be transferred across?
2296	 What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?
2297	What is the metric that is measures in this paper?
2298	Do they only test on one dataset?
2299	What baseline decoder do they use?
2300	Was evaluation metrics and criteria were used to evaluate the output of the cascaded multimodal speech translation?
2301	What dataset was used in this work?
2302	How do they evaluate the sentence representations?
2303	What are the two decoding functions?
2304	How is language modelling evaluated?
2305	Why there is only user study to evaluate the model?
2306	What datasets are used to evaluate the model?
2307	How did they gather the data?
2308	What are the domains covered in the dataset?
2309	What is their baseline?
2310	Do they use the cased or uncased BERT model?
2311	How are the two different models trained?
2312	How long is the dataset?
2313	How big are negative effects of proposed techniques on high-resource tasks?
2314	What datasets are used for experiments?
2315	Are this techniques used in training multilingual models, on what languages?
2316	What baselines non-adaptive baselines are used?
2317	What text sequences are associated with each vertex?
2318	How long does it take for the model to run?
2319	Do they report results only on English data?
2320	Which other unsupervised models are used for comparison?
2321	What metric is used to measure performance?
2322	How do the n-gram features incorporate compositionality?
2323	Which dataset do they use?
2324	How do Zipf and Herdan-Heap's laws differ?
2325	What was the best performing baseline?
2326	Which approaches did they use?
2327	What is the size of the dataset?
2328	Did they use a crowdsourcing platform for the summaries?
2329	How are the synthetic examples generated?
2330	Do they measure the number of created No-Arc long sequences?
2331	By how much does the new parser outperform the current state-of-the-art?
2332	Do they evaluate only on English datasets?
2333	What experimental evaluation is used?
2334	How is the architecture fault-tolerant?
2335	Which elements of the platform are modular?
2336	What is the source of memes?
2337	Is the dataset multimodal?
2338	How is each instance of the dataset annotated?
2339	Which dataset do they use for text modelling?
2340	Do they compare against state of the art text generation?
2341	How do they evaluate generated text quality?
2342	Was the system only evaluated over the second shared task?
2343	Could you tell me more about the metrics used for performance evaluation?
2344	which tasks are used in BLUE benchmark?
2345	What are the tasks that this method has shown improvements?
2346	Why does the model improve in monolingual spaces as well? 
2347	What are the categories being extracted?
2348	Do the authors test their annotation projection techniques on tasks other than AMR?
2349	How is annotation projection done when languages have different word order?
2350	What is the reasoning method that is used?
2351	What KB is used in this work?
2352	What's the precision of the system?
2353	How did they measure effectiveness?
2354	Which of the two ensembles yields the best performance?
2355	What are the two ways of ensembling BERT and E-BERT?
2356	How is it determined that a fact is easy-to-guess?
2357	How is dependency parsing empirically verified?
2358	How are different network components evaluated?
2359	What are the performances obtained for PTB and CTB?
2360	What are the models used to perform constituency and dependency parsing?
2361	Is the proposed layer smaller in parameters than a Transformer?
2362	What is the new initialization method proposed in this paper?
2363	How was a quality control performed so that the text is noisy but the annotations are accurate?
2364	Is it a neural model? How is it trained?
2365	How do people engage in Twitter threads on different types of news?
2366	How are the clusters related to security, violence and crime identified?
2367	What are the features of used to customize target user interaction? 
2368	How they evaluate quality of generated output?
2369	What automated metrics authors investigate?
2370	What supervised models are experimented with?
2371	Who annotated the data?
2372	What are the four forums the data comes from?
2373	How do they obtain parsed source sentences?
2374	What kind of encoders are used for the parsed source sentence?
2375	Whas is the performance drop of their model when there is no parsed input?
2376	How were their results compared to state-of-the-art?
2377	What supports the claim that injected CNN into recurent units will enhance ability of the model to catch local context and reduce ambiguities?
2378	How is CNN injected into recurent units?
2379	Are there some results better than state of the art on these tasks?
2380	Do experiment results show consistent significant improvement of new approach over traditional CNN and RNN models?
2381	What datasets are used for testing sentiment classification and reading comprehension?
2382	So we do not use pre-trained embedding in this case?
2383	How are sentence embeddings incorporated into the speech recognition system?
2384	How different is the dataset size of source and target?
2385	How do you find the entity descriptions?
2386	How is OpenBookQA different from other natural language QA?
2387	At what text unit/level were documents processed?
2388	What evaluation metric were used for presenting results? 
2389	Was the structure of regulatory filings exploited when training the model? 
2390	What type of documents are supported by the annotation platform?
2391	What are the state-of-the-art models for the task?
2392	Which datasets are used for evaluation?
2393	What are the strong baselines you have?
2394	What are causal attribution networks?
2395	How accurate is their predictive model?
2396	How large language sets are able to be explored using this approach?
2397	how did they ask if a tweet was racist?
2398	What other cross-lingual approaches is the model compared to?
2399	What languages are explored?
2400	How many human subjects were used in the study?
2401	How does the model compute the likelihood of executing to the correction semantic denotation?
2402	Which conventional alignment models do they use as guidance?
2403	Which dataset do they use?
2404	On average, by how much do they reduce the diarization error?
2405	Do they compare their algorithm to voting without weights?
2406	How do they assign weights between votes in their DOVER algorithm?
2407	What are state of the art methods authors compare their work with? 
2408	What are the baselines model?
2409	What is the architecture of the model?
2410	What languages are explored in the work?
2411	What is the state-of-the-art neural coreference resolution model?
2412	How much improvement do they get?
2413	How large is the dataset?
2414	What features do they extract?
2415	What they use as a metric of finding hot spots in meeting?
2416	Is this approach compared to some baseline?
2417	How big is ICSI meeting corpus?
2418	What annotations are available in ICSI meeting corpus?
2419	Is such bias caused by bad annotation?
2420	How do they determine similar environments for fragments in their data augmentation scheme?
2421	Do they experiment with language modeling on large datasets?
2422	Which languages do they test on?
2423	What limitations are mentioned?
2424	What examples of applications are mentioned?
2425	Did they crowdsource the annotations?
2426	Why they conclude that the usage of Gated-Attention provides no competitive advantage against concatenation in this setting?
2427	What was the best team's system?
2428	What are the baselines?
2429	What semantic features help in detecting whether a piece of text is genuine or generated? of 
2430	Which language models generate text that can be easier to classify as genuine or generated?
2431	Is the assumption that natural language is stationary and ergodic valid?
2432	Which models do they try out?
2433	Do they compare executionttime of their model against other models?
2434	What is the memory footprint decrease of their model in comparison to other models?
2435	What architectural factors were investigated?
2436	Any other bias may be detected?
2437	What is the introduced meta-embedding method introduced in this paper?
2438	How long are dialogue recordings used for evaluation?
2439	What do the models that they compare predict?
2440	What SMT models did they look at?
2441	Which NMT models did they experiment with?
2442	How big PIE datasets are obtained from dictionaries?
2443	What compleentary PIE extraction methods are used to increase reliability further?
2444	Are PIEs extracted automatically subjected to human evaluation?
2445	What dictionaries are used for automatic extraction of PIEs?
2446	Are experiments performed with any other pair of languages, how did proposed method perform compared to other models?
2447	Is pivot language used in experiments English or some other language?
2448	What are multilingual models that were outperformed in performed experiment?
2449	What are the common captioning metrics?
2450	Which English domains do they evaluate on?
2451	What is the road exam metric?
2452	What are the competing models?
2453	How is the input triple translated to a slot-filling task?
2454	Is model compared against state of the art models on these datasets?
2455	How is octave convolution concept extended to multiple resolutions and octaves?
2456	Does this paper address the variation among English dialects regarding these hedges?
2457	On which dataset is model trained?
2458	How is module that analyzes behavioral state trained?
2459	Can the model add new relations to the knowledge graph, or just new entities?
2460	How large is the dataset?
2461	Why is a Gaussian process an especially appropriate method for this classification problem?
2462	Do the authors do manual evaluation?
2463	What datasets did they use?
2464	Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?
2465	What is the training and test data used?
2466	Was performance of the weakly-supervised model compared to the performance of a supervised model?
2467	Do the tweets come from a specific region?
2468	Did they experiment with the corpus?
2469	What writing styles are present in the corpus?
2470	How did they determine the distinct classes?
2471	Do they jointly tackle multiple tagging problems?
2472	How many parameters does their CNN have?
2473	How do they confirm their model working well on out-of-vocabulary problems?
2474	What approach does this work propose for the new task?
2475	What is the new task proposed in this work?
2476	Which news organisations are the headlines sourced from?
2477	What meta-information is being transferred?
2478	What datasets are used to evaluate the approach?
2479	Does their solution involve connecting images and text?
2480	Which model do they use to generate key messages?
2481	What experiments they perform to demonstrate that their approach leads more accurate region based representations?
2482	How they indentify conceptual neighbours?
2483	What experiment result led to conclussion that reducing the number of layers of the decoder does not matter much?
2484	How much is performance hurt when using too small amount of layers in encoder?
2485	What was previous state of the art model for automatic post editing?
2486	What neural machine translation models can learn in terms of transfer learning?
2487	Did they experiment on the proposed task?
2488	Is annotation done manually?
2489	How large is the proposed dataset?
2490	How large is the dataset?
2491	How is the dataset created?
2492	What is binary variational dropout?
2493	Which strategies show the most promise in deterring these attacks?
2494	What are baseline models on WSJ eval92 and LibriSpeech test-clean?
2495	Do they use the same architecture as LSTM-s and GRUs with just replacing with the LAU unit?
2496	So this paper turns unstructured text inputs to parameters that GNNs can read?
2497	What other models are compared to the Blending Game?
2498	What empirical data are the Blending Game predictions compared to?
2499	How does the semi-automatic construction process work?
2500	Does the paper report translation accuracy for an automatic translation model for Tunisian to Arabish words?
2501	Did participants behave unexpectedly?
2502	Was this experiment done in a lab?
2503	How long is new model trained on 3400 hours of data?
2504	How much does HAS-QA improve over baselines?
2505	What does "explicitly leverages their probabilistic correlation to guide the training process of both models" mean?
2506	How does this compare to contextual embedding methods?
2507	Does the new system utilize pre-extracted bounding boxes and/or features?
2508	To which previous papers does this work compare its results?
2509	Do they consider other tasks?
2510	What were the model's results on flood detection?
2511	What dataset did they use?
2512	What exactly is new about this stochastic gradient descent algorithm?
2513	What codemixed language pairs are evaluated?
2514	How do they compress the model?
2515	What is the multilingual baseline?
2516	Which features do they use?
2517	By how much do they outperform state-of-the-art solutions on SWDA and MRDA?
2518	What type and size of word embeddings were used?
2519	What data was used to build the word embeddings?
2520	How are templates discovered from training data?
2521	What is WNGT 2019 shared task?
2522	Do they use pretrained word representations in their neural network models?
2523	How do they combine the two proposed neural network models?
2524	Which dataset do they evaluate grammatical error correction on?
2525	How many users/clicks does their search engine have?
2526	what was their baseline comparison?
2527	Was any variation in results observed based on language typology?
2528	Does the work explicitly study the relationship between model complexity and linguistic structure encoding?
2529	Which datasets are used in this work?
2530	Does the training dataset provide logical form supervision?
2531	What is the difference between the full test set and the hard test set?
2532	How is the discriminative training formulation different from the standard ones?
2533	How are the two datasets artificially overlapped?
2534	What baseline system is used?
2535	What type of lexical, syntactic, semantic and polarity features are used?
2536	How does nextsum work?
2537	Can the approach be generalized to other technical domains as well? 
2538	How many tweets were manually labelled? 
2539	What dataset they use for evaluation?
2540	What is the source of the tables?
2541	Which regions of the United States do they consider?
2542	Why did they only consider six years of published books?
2543	What state-of-the-art general-purpose pretrained models are made available under the unified API? 
2544	How is performance measured?
2545	What models are included in the toolkit?
2546	Is there any human evaluation involved in evaluating this famework?
2547	How big is multilingual dataset?
2548	How big is dataset used for fine-tuning BERT?
2549	How big are datasets for 2019 Amazon Alexa competition?
2550	What is novel in author's approach?
2551	How large is the Dialog State Tracking Dataset?
2552	What dataset is used for train/test of this method?
2553	How much is the gap between using the proposed objective and using only cross-entropy objective?
2554	What is the multi-instance learning?
2555	How many domains of ontologies do they gather data from?
2556	How is the semi-structured knowledge base created?
2557	what is the practical application for this paper?
2558	Do they use a neural model for their task?
2559	What's the method used here?
2560	By how much does their method outperform state-of-the-art OOD detection?
2561	What are dilated convolutions?
2562	what was the evaluation metrics studied in this work?
2563	Do they analyze ELMo?
2564	what are the three methods presented in the paper?
2565	what datasets did the authors use?
2566	What are three possible phases for language formation?
2567	How many parameters does the model have?
2568	Do the experiments explore how various architectures and layers contribute towards certain decisions?
2569	What social media platform does the data come from?
2570	How much performance improvements they achieve on SQuAD?
2571	Do the authors perform experiments using their proposed method?
2572	What NLP tasks do the authors evaluate feed-forward networks on?
2573	What are three challenging tasks authors evaluated their sequentially aligned representations?
2574	What is the difference in findings of Buck et al? It looks like the same conclusion was mentioned in Buck et al..
2575	What is the baseline?
2576	What is the unsupervised task in the final layer?
2577	How many supervised tasks are used?
2578	What is the network architecture?
2579	Is the proposed model more sensitive than previous context-aware models too?
2580	In what ways the larger context is ignored for the models that do consider larger context?
2581	What does recurrent deep stacking network do?
2582	Does the latent dialogue state heklp their model?
2583	Do the authors test on datasets other than bAbl?
2584	What is the reward model for the reinforcement learning appraoch?
2585	Does this paper propose a new task that others can try to improve performance on?
2586	What knowledge base do they use?
2587	How big is their dataset?
2588	What task do they evaluate on?
2589	Do some pretraining objectives perform better than others for sentence level understanding tasks?
2590	Did the authors try stacking multiple convolutional layers?
2591	How many feature maps are generated for a given triple?
2592	How does the number of parameters compare to other knowledge base completion models?
2593	which multilingual approaches do they compare with?
2594	what are the pivot-based baselines?
2595	which datasets did they experiment with?
2596	what language pairs are explored?
2597	what ner models were evaluated?
2598	what is the source of the news sentences?
2599	did they use a crowdsourcing platform for manual annotations?
2600	what are the topics pulled from Reddit?
2601	What predictive model do they build?
2602	What accuracy does the proposed system achieve?
2603	What crowdsourcing platform is used?
2604	How do they match words before reordering them?
2605	On how many language pairs do they show that preordering assisting language sentences helps translation quality?
2606	Which dataset(s) do they experiment with?
2607	Which information about text structure is included in the corpus?
2608	Which information about typography is included in the corpus?
2609	On which benchmarks they achieve the state of the art?
2610	What they use in their propsoed framework?
2611	What does KBQA abbreviate for
2612	What is te core component for KBQA?
2613	What experiments are proposed to test that upper layers produce context-specific embeddings?
2614	How do they calculate a static embedding for each word?
2615	What is the performance of BERT on the task?
2616	What are the other algorithms tested?
2617	Does BERT reach the best performance among all the algorithms compared?
2618	What are the clinical datasets used in the paper?
2619	how is model compactness measured?
2620	what was the baseline?
2621	what evaluation metrics were used?
2622	what datasets did they use?
2623	What is the interannotator agreement for the human evaluation?
2624	Who were the human evaluators used?
2625	Is the template-based model realistic?  
2626	Is the student reflection data very different from the newspaper data?  
2627	What is the recent abstractive summarization method in this paper?
2628	Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models?  
2629	What state-of-the-art compression techniques were used in the comparison?
2630	What evaluations methods do they take?
2631	What is the size of the dataset?
2632	Which methods are considered to find examples of biases and unwarranted inferences??
2633	What biases are found in the dataset?
2634	What discourse relations does it work best/worst for?
2635	How much does this model improve state-of-the-art?
2636	Where is a question generation model used?
2637	Were any of these tasks evaluated in any previous work?
2638	Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?
2639	Which demographic dimensions of people do they obtain?
2640	How do they obtain psychological dimensions of people?
2641	What is the baseline?
2642	Is the data de-identified?
2643	What embeddings are used?
2644	What datasets did they use for evaluation?
2645	On top of BERT does the RNN layer work better or the transformer layer?
2646	How was this data collected?
2647	What is the average length of dialog?
2648	How are models evaluated in this human-machine communication game?
2649	How many participants were trying this communication game?
2650	What user variations have been tested?
2651	What are the baselines used?
2652	Do they use off-the-shelf NLP systems to build their assitant?
2653	How does the IPA label data after interacting with users?
2654	What kind of repetitive and time-consuming activities does their assistant handle?
2655	How was the audio data gathered?
2656	What is the GhostVLAD approach?
2657	Which 7 Indian languages do they experiment with?
2658	What datasets do they evaluate on?
2659	Do they evaluate only on English datasets?
2660	What is the invertibility condition?
2661	Do they show on which examples how conflict works better than attention?
2662	Which neural architecture do they use as a base for their attention conflict mechanisms?
2663	On which tasks do they test their conflict method?
2664	Do they use graphical models?
2665	What are the sources of the datasets?
2666	What metric is used for evaluation?
2667	Which eight NER tasks did they evaluate on?
2668	What in-domain text did they use?
2669	Does their framework automatically optimize for hyperparameters?
2670	Does their framework always generate purely attention-based models?
2671	Do they test their framework performance on commonly used language pairs, such as English-to-German?
2672	Which languages do they test on for the under-resourced scenario?
2673	Are the automatically constructed datasets subject to quality control?
2674	Do they focus on Reading Comprehension or multiple choice question answering?
2675	After how many hops does accuracy decrease?
2676	How do they control for annotation artificats?
2677	Is WordNet useful for taxonomic reasoning for this task?
2678	How do they perform multilingual training?
2679	What languages are evaluated?
2680	Does the model have attention?
2681	What architecture does the decoder have?
2682	What architecture does the encoder have?
2683	What is MSD prediction?
2684	What type of inflections are considered?
2685	Do they use attention?
2686	What other models do they compare to?
2687	What is the architecture of the span detector?
2688	What evaluation metric do they use?
2689	What are the results from these proposed strategies?
2690	What are the baselines?
2691	What are the two new strategies?
2692	Do they report results only on English data?
2693	How much better than the baseline is LiLi?
2694	What baseline is used in the experiments?
2695	In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?
2696	What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation? 
2697	What are the components of the general knowledge learning engine?
2698	How many labels do the datasets have?
2699	What is the architecture of the model?
2700	What are the baseline methods?
2701	What are the source and target domains?
2702	Did they use a crowdsourcing platform for annotations?
2703	How do they deal with unknown distribution senses?
2704	Do they report results only on English data?
2705	What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?
2706	How id Depechemood trained?
2707	How are similarities and differences between the texts from violent and non-violent religious groups analyzed?
2708	How are prominent topics idenified in Dabiq and Rumiyah?
2709	Are the images from a specific domain?
2710	Which datasets are used?
2711	Which existing models are evaluated?
2712	How is diversity measured?
2713	What state-of-the-art deep neural network is used?
2714	What boundary assembling method is used?
2715	What are previous state of the art results?
2716	What is the model performance on target language reading comprehension?
2717	What source-target language pairs were used in this work? 
2718	What model is used as a baseline?  
2719	what does the model learn in zero-shot setting?
2720	Do they inspect their model to see if their model learned to associate image parts with words related to entities?
2721	Does their NER model learn NER from both text and images?
2722	Which types of named entities do they recognize?
2723	Can named entities in SnapCaptions be discontigious?
2724	How large is their MNER SnapCaptions dataset?
2725	What is masked document generation?
2726	Which of the three pretraining tasks is the most helpful?
2727	What useful information does attention capture?
2728	What datasets are used?
2729	In what cases is attention different from alignment?
2730	How do they calculate variance from the model outputs?
2731	How much data samples do they start with before obtaining the initial model labels?
2732	Which model do they use for end-to-end speech recognition?
2733	Which dataset do they use?
2734	Which baselines did they compare against?
2735	What baselines did they consider?
2736	What types of social media did they consider?
2737	How was the dataset annotated?
2738	Which classifiers are evaluated?
2739	What is the size of this dataset?
2740	Where does the data come from?
2741	What are method improvements of F1 for paraphrase identification?
2742	What are method's improvements of F1 for NER task for English and Chinese datasets?
2743	What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?
2744	How are weights dynamically adjusted?
2745	Ngrams of which length are aligned using PARENT?
2746	How many people participated in their evaluation study of table-to-text models?
2747	By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?
2748	Which stock market sector achieved the best performance?
2749	What languages pairs are used in machine translation?
2750	What sentiment classification dataset is used?
2751	What pooling function is used?
2752	Do they report results only on English?
2753	What neural network modules are included in NeuronBlocks?
2754	How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?
2755	what datasets did they use?
2756	what ml based approaches were compared?
2757	Is pre-training effective in their evaluation?
2758	What parallel corpus did they use?
2759	How much does their model outperform existing models?
2760	What do they mean by global and local context?
2761	What are the 18 propaganda techniques?
2762	What dataset was used?
2763	What was the baseline for this task?
2764	What is a second order co-ocurrence matrix?
2765	How many humans participated?
2766	What embedding techniques are explored in the paper?
2767	Do the authors also try the model on other datasets?
2768	What word level and character level model baselines are used?
2769	By how much do they improve the efficacy of the attention mechanism?
2770	How were the human judgements assembled?
2771	Did they only experiment with one language pair?
2772	Which other approaches do they compare their model with?
2773	What results do they achieve using their proposed approach?
2774	How do they combine a deep learning model with a knowledge base?
2775	What are the models used for the baseline of the three NLP tasks?
2776	How is non-standard pronunciation identified?
2777	Is it valid to presume a bad medical wikipedia article should not contain much domain-specific jargon?
2778	What novel PMI variants are introduced?
2779	What semantic and syntactic tasks are used as probes?
2780	What are the disadvantages to clipping negative PMI?
2781	Why are statistics from finite corpora unreliable?
2782	what is the domain of the corpus?
2783	what challenges are identified?
2784	what is the size of the speech corpus?
2785	Which two pairs of ERPs from the literature benefit from joint training?
2786	What datasets are used?
2787	which datasets did they experiment with?
2788	which languages are explored?
2789	Do they use number of votes as an indicator of preference?
2790	What does a node in the network approach repesent?
2791	Which dataset do they use?
2792	What kind of celebrities do they obtain tweets from?
2793	How did they extend LAMA evaluation framework to focus on negation?
2794	What summarization algorithms did the authors experiment with?
2795	What evaluation metrics were used for the summarization task?
2796	What clustering algorithms were used?
2797	What evaluation metrics are looked at for classification tasks?
2798	What methods were used for sentence classification?
2799	What is the average length of the sentences?
2800	What is the size of the real-life dataset?
2801	What are the language pairs explored in this paper?
2802	Do they experiment with the toolkits?
2803	Have they made any attempt to correct MRC gold standards according to their findings? 
2804	What features are absent from MRC gold standards that can result in potential lexical ambiguity?
2805	What modern MRC gold standards are analyzed?
2806	How does proposed qualitative annotation schema looks like?
2807	How many tweets were collected?
2808	What language is explored in this paper?
2809	What are the baselines?
2810	What is the attention module pretrained on?
2811	How long of dialog history is captured?
2812	What evaluation metrics were used?
2813	What was the score of the proposed model?
2814	What was the previous best model?
2815	Which datasets did they use for evaluation?
2816	What hyperparameters are explored?
2817	What Named Entity Recognition dataset is used?
2818	What sentiment analysis dataset is used?
2819	Do they test both skipgram and c-bow?
2820	What is the state-of-the-art model for the task?
2821	What is the strong baseline?
2822	what aspects of conversation flow do they look at?
2823	what debates dataset was used?
2824	what is the state of the art?
2825	what standard dataset were used?
2826	Do they perform error analysis?
2827	How do their results compare to state-of-the-art?
2828	What is the Random Kitchen Sink approach?
2829	what are the baseline systems?
2830	What word embeddings do they test?
2831	How do they define similar equations?
2832	What evaluation criteria and metrics were used to evaluate the generated text?
2833	Do they evaluate only on English datasets?
2834	What are the three steps to feature elimination?
2835	How is the dataset annotated?
2836	What dataset is used for this study?
2837	what were their performance results?
2838	where did they obtain the annotated clinical notes from?
2839	Which architecture do they use for the encoder and decoder?
2840	How does their decoder generate text?
2841	Which dataset do they use?
2842	What model is used to encode the images?
2843	How is the sequential nature of the story captured?
2844	Is the position in the sequence part of the input?
2845	Do the decoder LSTMs all have the same weights?
2846	Is fine-tuning required to incorporate these embeddings into existing models?
2847	How are meaningful chains in the graph selected?
2848	Do the authors also analyze transformer-based architectures?
2849	Do they remove seasonality from the time series?
2850	What is the dimension of the embeddings?
2851	What dataset is used to train the model?
2852	What is the previous state of the art?
2853	Which text embedding methodologies are used?
2854	Which race and gender are given higher sentiment intensity predictions?
2855	What criteria are used to select the 8,640 English sentences?
2856	what were the baselines?
2857	what competitive results did they obtain?
2858	By how much is performance improved with multimodality?
2859	Is collected multimodal in cabin dataset public?
2860	What is the performance reported for the best models in the VLSP 2018 and VLSP 2019 challenges?
2861	Is the model tested against any baseline?
2862	What is the language model combination technique used in the paper?
2863	What are the deep learning architectures used in the task?
2864	How much is performance improved on NLI?
2865	Do they train their model starting from a checkpoint?
2866	What BERT model do they test?
2867	What downstream tasks are evaluated?
2868	What is active learning?
2869	what was the baseline?
2870	How is segmentation quality evaluated?
2871	How do they compare lexicons?
2872	Is it possible to convert a cloze-style questions to a naturally-looking questions?
2873	How larger are the training sets of these versions of ELMo compared to the previous ones?
2874	What is the improvement in performance for Estonian in the NER task?
2875	what is the state of the art on WSJ?
2876	How did they obtain the OSG dataset?
2877	How large is the Twitter dataset?
2878	what is the size of the augmented dataset?
2879	How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?
2880	What dataset do they use to evaluate their method?
2881	Why are current ELS's not sufficiently effective?
2882	What is the best model?
2883	How many sentences does the dataset contain?
2884	Do the authors train a Naive Bayes classifier on their dataset?
2885	What is the baseline?
2886	Which machine learning models do they explore?
2887	What is the size of the dataset?
2888	What is the source of their dataset?
2889	Do they try to use byte-pair encoding representations?
2890	How many different types of entities exist in the dataset?
2891	How big is the new Nepali NER dataset?
2892	What is the performance improvement of the grapheme-level representation model over the character-level model?
2893	Which models are used to solve NER for Nepali?
2894	What language(s) is/are represented in the dataset?
2895	What baseline model is used?
2896	Which variation provides the best results on this dataset?
2897	What are the different variations of the attention-based approach which are examined?
2898	What dataset is used for this work?
2899	What types of online harassment are studied?
2900	What was the baseline?
2901	What were the datasets used in this paper?
2902	Is car-speak language collection of abstract features that classifier is later trained on?
2903	Is order of "words" important in car speak language?
2904	What are labels in car speak language dataset?
2905	How big is dataset of car-speak language?
2906	What is the performance of classifiers?
2907	What classifiers have been trained?
2908	How does car speak pertains to a car's physical attributes?
2909	What topic is covered in the Chinese Facebook data? 
2910	How many layers does the UTCNN model have?
2911	What topics are included in the debate data?
2912	What is the size of the Chinese data?
2913	Did they collected the two datasets?
2914	What are the baselines?
2915	What transfer learning tasks are evaluated?
2916	What metrics are used for the STS tasks?
2917	How much time takes its training?
2918	How many GPUs are used for the training of SBERT?
2919	How are the siamese networks trained?
2920	What other sentence embeddings methods are evaluated?
2921	What is the average length of the title text?
2922	Which pretrained word vectors did they use?
2923	What evaluation metrics are used?
2924	Which shallow approaches did they experiment with?
2925	Where do they obtain the news videos from?
2926	What is the source of the news articles?
2927	which non-english language had the best performance?
2928	which non-english language was the had the worst results?
2929	what datasets were used in evaluation?
2930	what are the baselines?
2931	how did the authors translate the reviews to other languages?
2932	what dataset was used for training?
2933	How do they demonstrate that this type of EEG has discriminative information about the intended articulatory movements responsible for speech?
2934	What are the five different binary classification tasks?
2935	How was the spatial aspect of the EEG signal computed?
2936	What data was presented to the subjects to elicit event-related responses?
2937	How many electrodes were used on the subject in EEG sessions?
2938	How many subjects does the EEG data come from?
2939	Do they report results only on English data?
2940	What type of classifiers are used?
2941	Which real-world datasets are used?
2942	How are the interpretability merits of the approach demonstrated?
2943	How are the accuracy merits of the approach demonstrated?
2944	How is the keyword specific expectation elicited from the crowd?
2945	Does the paper provide any case studies to illustrate how one can use Macaw for CIS research?
2946	What functionality does Macaw provide?
2947	What is a wizard of oz setup?
2948	What interface does Macaw currently have?
2949	What modalities are supported by Macaw?
2950	What are the different modules in Macaw?
2951	Do they report results only on English data?
2952	What baseline model is used?
2953	What news article sources are used?
2954	How do they determine the exact section to use the input article?
2955	What features are used to represent the novelty of news articles to entity pages?
2956	What features are used to represent the salience and relative authority of entities?
2957	Do they experiment with other tasks?
2958	What baselines do they introduce?
2959	How large is the corpus?
2960	How was annotation performed?
2961	How many documents are in the new corpus?
2962	What baseline systems are proposed?
2963	How did they obtain the dataset?
2964	What activation function do they use in their model?
2965	What baselines do they compare to?
2966	How are chunks defined?
2967	What features are extracted?
2968	How many layers does their model have?
2969	Was the approach used in this work to detect fake news fully supervised?
2970	Based on this paper, what is the more predictive set of features to detect fake news?
2971	How big is the dataset used in this work?
2972	How is a "chunk of posts" defined in this work?
2973	What baselines were used in this work?
2974	What is the performance of their method?
2975	Which evaluation methods are used?
2976	What dataset is used in this paper?
2977	Which other methods do they compare with?
2978	How are sentences selected from the summary graph?
2979	What models are used in the experiment?
2980	What are the differences between this dataset and pre-existing ones?
2981	In what language are the tweets?
2982	What is the size of the new dataset?
2983	What kinds of offensive content are explored?
2984	What is the best performing model?
2985	How many annotators participated?
2986	What is the definition of offensive language?
2987	What are the three layers of the annotation scheme?
2988	How long is the dataset for each step of hierarchy?
2989	Do the authors report results only on English data?
2990	In the proposed metric, how is content relevance measured?
2991	What different correlations result when using different variants of ROUGE scores?
2992	What manual Pyramid scores are used?
2993	What is the common belief that this paper refutes? (c.f. 'contrary to the common belief, ROUGE is not much [sic] reliable'
2994	which existing strategies are compared?
2995	what dataset was used?
2996	what kinds of male and female words are looked at?
2997	how is mitigation of gender bias evaluated?
2998	what bias evaluation metrics are used?
2999	What kind of questions are present in the dataset?
3000	What baselines are presented?
3001	What tasks were evaluated?
3002	What language are the reviews in?
3003	Where are the hotel reviews from?
3004	What was the baseline used?
3005	What are their results on both datasets?
3006	What textual patterns are extracted?
3007	Which annotated corpus did they use?
3008	Which languages are explored in this paper?
3009	what language does this paper focus on?
3010	what evaluation metrics did they use?
3011	by how much did their model improve?
3012	what state of the art methods did they compare with?
3013	what are the sizes of both datasets?
3014	What are the distinctive characteristics of how Arabic speakers use offensive language?
3015	How did they analyze which topics, dialects and gender are most associated with tweets?
3016	How many annotators tagged each tweet?
3017	How many tweets are in the dataset?
3018	In what way is the offensive dataset not biased by topic, dialect or target?
3019	What experiments are conducted?
3020	What is the combination of rewards for reinforcement learning?
3021	What are the difficulties in modelling the ironic pattern?
3022	How did the authors find ironic data on twitter?
3023	Who judged the irony accuracy, sentiment preservation and content preservation?
3024	How were the tweets annotated?
3025	Which SVM approach resulted in the best performance?
3026	What are hashtag features?
3027	How many tweets did they collect?
3028	Which sports clubs are the targets?
3029	Does this method help in sentiment classification task improvement?
3030	For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMo’s embedding?
3031	What are the black-box probes used?
3032	What are improvements for these two approaches relative to ELMo-only baselines?
3033	Which syntactic features are obtained automatically on downstream task data?
3034	Do they report results only on English data?
3035	What baseline approaches does this approach out-perform?
3036	What datasets are used?
3037	What alternative to Gibbs sampling is used?
3038	How does this model overcome the assumption that all words in a document are generated from a single event?
3039	How many users do they look at?
3040	What do they mean by a person's industry?
3041	What model did they use for their system?
3042	What social media platform did they look at?
3043	What are the industry classes defined in this paper?
3044	Do they report results only on English data?
3045	What are the hyperparameters of the bi-GRU?
3046	What baseline is used?
3047	What data is used in experiments?
3048	What meaningful information does the GRU model capture, which traditional ML models do not?
3049	What is the approach of previous work?
3050	Is the lexicon the same for all languages?
3051	How do they obtain the lexicon?
3052	What evaluation metric is used?
3053	Which languages are similar to each other?
3054	Which datasets are employed for South African languages LID?
3055	Does the paper report the performance of a baseline model on South African languages LID?
3056	What are the languages represented in the DSL datasets? 
3057	Does the algorithm improve on the state-of-the-art methods?
3058	What background knowledge do they leverage?
3059	What are the three regularization terms?
3060	What NLP tasks do they consider?
3061	How do they define robustness of a model?
3062	Are the annotations automatic or manually created?
3063	Do the errors of the model reflect linguistic similarity between different L1s?
3064	Is the dataset balanced between speakers of different L1s?
3065	How long are the essays on average?
3066	How large are the textual descriptions of entities?
3067	What neural models are used to encode the text?
3068	What baselines are used for comparison?
3069	What datasets are used to evaluate this paper?
3070	Which approach out of two proposed in the paper performed better in experiments?
3071	What classification baselines are used for comparison?
3072	What TIMIT datasets are used for testing?
3073	How does this approach compares to the state-of-the-art results on these tasks?
3074	What state-of-the-art results are achieved?
3075	What baselines do they compare with?
3076	What datasets are used in evaluation?
3077	What is the tagging scheme employed?
3078	How they extract "structured answer-relevant relation"?
3079	How big are significant improvements?
3080	What metrics do they use?
3081	On what datasets are experiments performed?
3082	What was the baseline model?
3083	What dataset did they use?
3084	What was their highest recall score?
3085	What was their highest MRR score?
3086	Does their model suffer exhibit performance drops when incorporating word importance?
3087	How do they measure which words are under-translated by NMT models?
3088	How do their models decide how much improtance to give to the output words?
3089	Which model architectures do they test their word importance approach on?
3090	Do they compare human-level performance to model performance for their dataset?
3091	What are the weaknesses found by non-expert annotators of current state-of-the-art NLI models?
3092	What data sources do they use for creating their dataset?
3093	Do they use active learning to create their dataset?
3094	Do the hashtag and SemEval datasets contain only English data?
3095	What current state of the art method was used for comparison?
3096	What set of approaches to hashtag segmentation are proposed?
3097	How is the dataset of hashtags sourced?
3098	How big is their created dataset?
3099	Which data do they use as a starting point for the dialogue dataset?
3100	What labels do they create on their dataset?
3101	How do they select instances to their hold-out test set?
3102	Which models/frameworks do they compare to?
3103	Which classification algorithm do they use for s2sL?
3104	Up to how many samples do they experiment with?
3105	Do they use pretrained models?
3106	Do they report results only on English datasets?
3107	How do the authors examine whether a model is robust to noise or not?
3108	What type of model is KAR?
3109	Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?
3110	What type of system does the baseline classification use?
3111	What experiments were carried out on the corpus?
3112	How many annotators tagged each text?
3113	Where did the texts in the corpus come from?
3114	What is the previous state-of-the-art in summarization?
3115	What dataset do they use?
3116	What other models do they compare to?
3117	What language model architectures are used?
3118	What are the user-defined keywords?
3119	Does the method achieve sota performance on this dataset?
3120	What are the baselines used in the paper?
3121	What is the size of the Airbnb?
3122	How better is performance compared to previous state-of-the-art models?
3123	How does Gaussian-masked directional multi-head attention works?
3124	What is meant by closed test setting?
3125	What are strong baselines model is compared to?
3126	Does the dataset feature only English language data?
3127	What additional features and context are proposed?
3128	What learning models are used on the dataset?
3129	What examples of the difficulties presented by the context-dependent nature of online aggression do they authors give?
3130	Do they report results only on English data?
3131	What evidence do the authors present that the model can capture some biases in data annotation and collection?
3132	Which publicly available datasets are used?
3133	What baseline is used?
3134	What new fine-tuning methods are presented?
3135	What are the existing biases?
3136	What biases does their model capture?
3137	What existing approaches do they compare to?
3138	What is the benchmark dataset?
3139	What are the two neural embedding models?
3140	which neural embedding model works better?
3141	What is the degree of dimension reduction of the efficient aggregation method?
3142	For which languages do they build word embeddings for?
3143	How do they evaluate their resulting word embeddings?
3144	What types of subwords do they incorporate in their model?
3145	Which matrix factorization methods do they use?
3146	Do they report results only on English data?
3147	What experiments do they use to quantify the extent of interpretability?
3148	Along which dimension do the semantically related words take larger values?
3149	What is the additive modification to the objective function?
3150	Which dataset do they use?
3151	Do they evaluate their learned representations on downstream tasks?
3152	Which representation learning architecture do they adopt?
3153	How do they encourage understanding of literature as part of their objective function?
3154	What are the limitations of existing Vietnamese word segmentation systems?
3155	Why challenges does word segmentation in Vietnamese pose?
3156	How successful are the approaches used to solve word segmentation in Vietnamese?
3157	Which approaches have been applied to solve word segmentation in Vietnamese?
3158	Which two news domains are country-independent?
3159	How is the political bias of different sources included in the model?
3160	What are the two large-scale datasets used?
3161	What are the global network features which quantify different aspects of the sharing process?
3162	Which datasets are used for evaluation?
3163	What previous methods is their model compared to?
3164	Did they use a crowdsourcing platform?
3165	How was the dataset collected?
3166	Which datasets do they use?
3167	How effective is their NCEL approach overall?
3168	How do they verify generalization ability?
3169	Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?
3170	Do the authors mention any downside of lemmatizing input before training ELMo?
3171	What other examples of morphologically-rich languages do the authors give?
3172	Why is lemmatization not necessary in English?
3173	How big was the corpora they trained ELMo on?
3174	What metrics are used for evaluation?
3175	Do they use pretrained embeddings?
3176	What dataset is used?
3177	What is a bifocal attention mechanism?
3178	What does the "sensitivity" quantity denote?
3179	What end tasks do they evaluate on?
3180	What is a semicharacter architecture?
3181	Do they experiment with offering multiple candidate corrections and voting on the model output, since this seems highly likely to outperform a one-best correction?
3182	Why is the adversarial setting appropriate for misspelling recognition?
3183	Why do they experiment with RNNs instead of transformers for this task?
3184	How do the backoff strategies work?
3185	What baseline model is used?
3186	Which additional latent variables are used in the model?
3187	Which parallel corpora are used?
3188	Overall, does having parallel data improve semantic role induction across multiple languages?
3189	Do they add one latent variable for each language pair in their Bayesian model?
3190	What does an individual model consist of?
3191	Do they improve on state-of-the-art semantic role induction?
3192	how many tags do they look at?
3193	which algorithm was the highest performer?
3194	how is diversity measured?
3195	how large is the vocabulary?
3196	what dataset was used?
3197	what algorithms did they use?
3198	How does their ensemble method work?
3199	How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset?
3200	How do they show there is space for further improvement?
3201	Do they report results only on English data?
3202	What argument components do the ML methods aim to identify?
3203	Which machine learning methods are used in experiments?
3204	How is the data in the new corpus come sourced?
3205	What argumentation phenomena encounter in actual data are now accounted for by this work?
3206	What challenges do different registers and domains pose to this task?
3207	who transcribed the corpus?
3208	how was the speech collected?
3209	what accents are present in the corpus?
3210	what evaluation protocols are provided?
3211	what age range is in the data?
3212	what is the source of the data?
3213	what topics did they label?
3214	did they compare with other extractive summarization methods?
3215	what datasets were used?
3216	what levels of document preprocessing are looked at?
3217	what keyphrase extraction models were reassessed?
3218	how many articles are in the dataset?
3219	Is this dataset publicly available for commercial use?
3220	How many different phenotypes are present in the dataset?
3221	What are 10 other phenotypes that are annotated?
3222	What are the state of the art models?
3223	Which benchmark datasets are used?
3224	What are the network's baseline features?
3225	What tasks are used for evaluation?
3226	HOw does the method perform compared with baselines?
3227	How does their model improve interpretability compared to softmax transformers?
3228	What baseline method is used?
3229	What details are given about the Twitter dataset?
3230	What details are given about the movie domain dataset?
3231	Which hand-crafted features are combined with word2vec?
3232	What word-based and dictionary-based feature are used?
3233	How are the supervised scores of the words calculated?
3234	what dataset was used?
3235	how many total combined features were there?
3236	what pretrained word embeddings were used?
3237	What evaluation metrics did look at?
3238	What datasets are used?
3239	What is the state of the art described in the paper?
3240	What GAN models were used as baselines to compare against?
3241	How much improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)?
3242	Is the discriminator's reward made available at each step to the generator?
3243	What is the algorithm used to create word embeddings?
3244	What is the corpus used for the task?
3245	How is evaluation performed?
3246	What is a normal reading paradigm?
3247	Did they experiment with this new dataset?
3248	What kind of sentences were read?
3249	why are their techniques cheaper to implement?
3250	what data simulation techniques were introduced?
3251	what is their explanation for the effectiveness of back-translation?
3252	what dataset is used?
3253	what language pairs are explored?
3254	what language is the data in?
3255	Does the experiments focus on a specific domain?
3256	how many training samples do you have for training?
3257	Do the answered questions measure for the usefulness of the answer?
3258	What profile metadata is used for this analysis?
3259	What are the organic and inorganic ways to show political affiliation through profile changes?
3260	How do profile changes vary for influential leads and their followers over the social movement?
3261	What evaluation metrics do they use?
3262	What is the size of this dataset?
3263	How do they determine if tweets have been used by journalists?
3264	how small of a dataset did they train on?
3265	what was their character error rate?
3266	which lstm models did they compare with?
3267	Do they use datasets with transcribed text or do they determine text from the audio?
3268	By how much does their model outperform the state of the art results?
3269	How do they combine audio and text sequences in their RNN?
3270	What was the baseline?
3271	By how much did they improve?
3272	What dataset did they use?
3273	What is the reported agreement for the annotation?
3274	How many annotators participated?
3275	What features are used?
3276	What future possible improvements are listed?
3277	Which qualitative metric are used for evaluation?
3278	What is quantitative improvement of proposed method (the best variant) w.r.t. baseline (the best variant)?
3279	How is "propaganda" defined for the purposes of this study?
3280	What metrics are used in evaluation?
3281	Which natural language(s) are studied in this paper?
3282	Do they report results only on English data?
3283	What objective function is used in the GAN?
3284	Which datasets are used?
3285	What metrics are used for evaluation?
3286	What natural language(s) are the recipes written in?
3287	What were their results on the new dataset?
3288	What are the baseline models?
3289	How did they obtain the interactions?
3290	Where do they get the recipes from?
3291	What were the baselines?
3292	Does RoBERTa outperform BERT?
3293	Which multiple datasets did they train on during joint training?
3294	What were the previously reported results?
3295	What is the size of SFU Review corpus?
3296	What is the size of bioScope corpus?
3297	Do they study numerical properties of their obtained vectors (such as orthogonality)?
3298	How do they score phrasal compositionality?
3299	Which translation systems do they compare against?
3300	what are their results on the constructed dataset?
3301	what evaluation metrics are reported?
3302	what civil field is the dataset about?
3303	what are the state-of-the-art models?
3304	what is the size of the real-world civil case dataset?
3305	what datasets are used in the experiment?
3306	Do they model semantics 
3307	How do they identify discussions of LGBTQ people in the New York Times?
3308	Do they analyze specific derogatory words?
3309	What is novel about their document-level encoder?
3310	What rouge score do they achieve?
3311	What are the datasets used for evaluation?
3312	What was their performance on emotion detection?
3313	Which existing benchmarks did they compare to?
3314	Which Facebook pages did they look at?
3315	LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?
3316	What is the benchmark dataset and is its quality high?
3317	How do they detect spammers?
3318	Do they use other evaluation metrics besides ROUGE?
3319	What is their ROUGE score?
3320	What are the baselines?
3321	What datasets do they use?
3322	What other factors affect the performance?
3323	What are the benchmark attacking methods?
3324	What domains are covered in the corpus?
3325	What is the architecture of their model?
3326	How was the dataset collected?
3327	Which languages are part of the corpus?
3328	How is the quality of the data empirically evaluated? 
3329	Is the data in CoVoST annotated for dialect?
3330	Is Arabic one of the 11 languages in CoVost?
3331	How big is Augmented LibriSpeech dataset?
3332	By how much does their best model outperform the state-of-the-art?
3333	Which dataset do they train their models on?
3334	How does their simple voting scheme work?
3335	Which variant of the recurrent neural network do they use?
3336	How do they obtain the new context represetation?
3337	Does the paper report the performance of the model for each individual language?
3338	What is the performance of the baseline?
3339	Did they pefrorm any cross-lingual vs single language evaluation?
3340	What was the performance of multilingual BERT?
3341	What annotations are present in dataset?
3342	What is an unordered text document, do these arise in real-world corpora?
3343	What kind of model do they use?
3344	Do they release a data set?
3345	Do they release code?
3346	Which languages do they evaluate on?
3347	Are the experts comparable to real-world users?
3348	Are the answers double (and not triple) annotated?
3349	Who were the experts used for annotation?
3350	What type of neural model was used?
3351	Were other baselines tested to compare with the neural baseline?
3352	Does the paper clearly establish that the challenges listed here exist in this dataset and task?
3353	Is this hashtag prediction task an established task, or something new?
3354	What is the word-level baseline?
3355	What other tasks do they test their method on?
3356	what is the word level baseline they compare to?
3357	What is the state of the art system mentioned?
3358	Do they incoprorate WordNet into the model?
3359	Is SemCor3.0 reflective of English language data in general?
3360	Do they use large or small BERT?
3361	How does the neural network architecture accomodate an unknown amount of senses per word?
3362	Which fonts are the best indicators of high quality?
3363	What kind of model do they use?
3364	Did they release their data set of academic papers?
3365	Do the methods that work best on academic papers also work best on Wikipedia?
3366	What is their system's absolute accuracy?
3367	Which is more useful, visual or textual features?
3368	Which languages do they use?
3369	How large is their data set?
3370	Where do they get their ground truth quality judgments?
3371	Which models did they experiment with?
3372	What were their best results on the benchmark datasets?
3373	What were the baselines?
3374	Which datasets were used?
3375	what datasets were used?
3376	what are the previous state of the art?
3377	what surface-level features are used?
3378	what linguistics features are used?
3379	what dataset statistics are provided?
3380	what is the size of their dataset?
3381	what crowdsourcing platform was used?
3382	how was the data collected?
3383	What is best performing model among author's submissions, what performance it had?
3384	What extracted features were most influencial on performance?
3385	Did ensemble schemes help in boosting peformance, by how much?
3386	Which basic neural architecture perform best by itself?
3387	What participating systems had better results than ones authors submitted?
3388	What is specific to multi-granularity and multi-tasking neural arhiteture design?
3389	Do they report results only on English data?
3390	What aspects of discussion are relevant to instructor intervention, according to the attention mechanism?
3391	What was the previous state of the art for this task?
3392	What type of latent context is used to predict instructor intervention?
3393	Do they report results only on English dataset?
3394	What dataset does this approach achieve state of the art results on?
3395	How much training data from the non-English language is used by the system?
3396	Is the system tested on low-resource languages?
3397	What languages are the model transferred to?
3398	How is the model transferred to other languages?
3399	What metrics are used for evaluation?
3400	What datasets are used for evaluation?
3401	what are the existing approaches?
3402	what dataset is used in this paper?
3403	How is keyphrase diversity measured?
3404	How was the StackExchange dataset collected?
3405	What does the TextWorld ACG dataset contain?
3406	What is the size of the StackExchange dataset?
3407	What were the baselines?
3408	What two metrics are proposed?
3409	Can the findings of this paper be generalized to a general-purpose task?
3410	Why does the proposed task a good proxy for the general-purpose sequence to sequence tasks?
3411	What was the baseline?
3412	What was their system's performance?
3413	What other political events are included in the database?
3414	What classifier did they use?
3415	What labels for antisocial events are available in datasets?
3416	What are two datasets model is applied to?
3417	What is the CORD-19 dataset?
3418	How large is the collection of COVID-19 literature?
3419	Which deep learning architecture do they use for sentence segmentation?
3420	How do they utilize unlabeled data to improve model representations?
3421	What is the McGurk effect?
3422	Are humans and machine learning systems fooled by the same kinds of illusions?
3423	how many humans evaluated the results?
3424	what was the baseline?
3425	what phenomena do they mention is hard to capture?
3426	by how much did the BLEU score improve?
3427	What is NER?
3428	Does the paper explore extraction from electronic health records?
3429	Does jiant involve datasets for the 50 NLU tasks?
3430	Is jiant compatible with models in any programming language?
3431	What models are used for painting embedding and what for language style transfer?
3432	What applicability of their approach is demonstrated by the authors?
3433	What limitations do the authors demnostrate of their model?
3434	How does final model rate on Likert scale?
3435	How big is English poem description of the painting dataset?
3436	What is best BLEU score of language style transfer authors got?
3437	How better does new approach behave than existing solutions?
3438	How is trajectory with how rewards extracted?
3439	On what Text-Based Games are experiments performed?
3440	How do the authors show that their learned policy generalize better than existing solutions to unseen games?
3441	How much is classification performance improved in experiments for low data regime and class-imbalance problems?
3442	What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?
3443	What subtasks did they participate in?
3444	What were the scores of their system?
3445	How was the training data translated?
3446	What dataset did they use?
3447	What other languages did they translate the data from?
3448	What semi-supervised learning is applied?
3449	How were the datasets annotated?
3450	What are the 12 languages covered?
3451	Does the corpus contain only English documents?
3452	What type of evaluation is proposed for this task?
3453	What baseline system is proposed?
3454	How were crowd workers instructed to identify important elements in large document collections?
3455	Which collections of web documents are included in the corpus?
3456	How do the authors define a concept map?
3457	Is the LSTM baseline a sub-word model?
3458	How is pseudo-perplexity defined?
3459	What is the model architecture used?
3460	How is the data used for training annotated?
3461	what quantitative analysis is done?
3462	what are the baselines?
3463	Do they report results only on English data?
3464	What machine learning and deep learning methods are used for RQE?
3465	by how much did nus outperform abus?
3466	what corpus is used to learn behavior?
3467	Which dataset has been used in this work?
3468	What can word subspace represent?
3469	How big are improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information?
3470	To what baseline models is proposed model compared?
3471	How big is dataset for testing?
3472	What existing dataset is re-examined and corrected for training?
3473	What are the qualitative experiments performed on benchmark datasets?
3474	How does this approach compare to other WSD approaches employing word embeddings?
3475	What tasks did they use to evaluate performance for male and female speakers?
3476	What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?
3477	Which corpora does this paper analyse?
3478	How many categories do authors define for speaker role?
3479	How big is imbalance in analyzed corpora?
3480	What are four major corpora of French broadcast?
3481	What did the best systems use for their model?
3482	What were their results on the classification and regression tasks
3483	Do the authors conduct experiments on the tasks mentioned?
3484	Did they collect their own datasets?
3485	What data do they look at?
3486	What language do they explore?
3487	Do they report results only on English datasets?
3488	Which hyperparameters were varied in the experiments on the four tasks?
3489	Which other hyperparameters, other than number of clusters are typically evaluated in this type of research?
3490	How were the cluster extracted? 
3491	what were the evaluation metrics?
3492	what are the state of the art methods?
3493	what english datasets were used?
3494	which chinese datasets were used?
3495	What were their distribution results?
3496	How did they determine fake news tweets?
3497	What is their definition of tweets going viral?
3498	What are the characteristics of the accounts that spread fake news?
3499	What is the threshold for determining that a tweet has gone viral?
3500	How is the ground truth for fake news established?
3501	What was the baseline?
3502	Which three discriminative models did they use?
3503	what NMT models did they compare with?
3504	Where does the ancient Chinese dataset come from?
3505	How many different characters were in dataset?
3506	How does dataset model character's profiles?
3507	How big is the difference in performance between proposed model and baselines?
3508	What baseline models are used?
3509	Was PolyReponse evaluated against some baseline?
3510	What metric is used to evaluate PolyReponse system?
3511	How does PolyResponse architecture look like?
3512	In what 8 languages is PolyResponse engine used for restourant search and booking system?
3513	Why masking words in the decoder is helpful?
3514	What is the ROUGE score of the highest performing model?
3515	How are the different components of the model trained? Is it trained end-to-end?
3516	When is this paper published?
3517	Can their indexing-based method be applied to create other QA datasets in other domains, and not just Wikipedia?
3518	Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?
3519	How many question types do they find in the datasets analyzed?
3520	How do they analyze contextual similaries across datasets?
3521	What were their performance results?
3522	What cyberbulling topics did they address?
3523	Were any of the pipeline components based on deep learning models?
3524	How is the effectiveness of this pipeline approach evaluated?
3525	What is the size of the parallel corpus used to train the model constraints?
3526	How does enforcing agreement between parse trees work across different languages?
3527	What datasets are used to assess the performance of the system?
3528	How is the vocabulary of word-like or phoneme-like units automatically discovered?
3529	IS the graph representation supervised?
3530	Is the G-BERT model useful beyond the task considered?
3531	How well did the baseline perform?
3532	What is the baseline?
3533	what methods were used to reduce data sparsity effects?
3534	what was the baseline?
3535	did they collect their own data?
3536	what japanese-vietnamese dataset do they use?
3537	How do they measure style transfer success?
3538	Do they introduce errors in the data or does the data already contain them?
3539	What error types is their model more reliable for?
3540	How does their parallel data differ in terms of style?
3541	How do they split text to obtain sentence levels?
3542	Do they experiment with their proposed model on any other dataset other than MovieQA?
3543	What is the difference of the proposed model with a standard RNN encoder-decoder?
3544	Does the model evaluated on NLG datasets or dialog datasets?
3545	What tasks do they experiment with?
3546	What is the meta knowledge specifically?
3547	Are there elements, other than pitch, that can potentially result in out of key converted singing?
3548	How is the quality of singing voice measured?
3549	what data did they use?
3550	what previous RNN models do they compare with?
3551	What are examples of these artificats?
3552	What are the languages they use in their experiment?
3553	Does the professional translation or the machine translation introduce the artifacts?
3554	Do they recommend translating the premise and hypothesis together?
3555	Is the improvement over state-of-the-art statistically significant?
3556	What are examples of these artifacts?
3557	What languages do they use in their experiments?
3558	How much higher quality is the resulting annotated data?
3559	How do they match annotators to instances?
3560	How much data is needed to train the task-specific encoder?
3561	What kind of out-of-domain data?
3562	Is an instance a sentence or an IE tuple?
3563	Who are the crowdworkers?
3564	Which toolkits do they use?
3565	Which sentiment class is the most accurately predicted by ELS systems?
3566	Is datasets for sentiment analysis balanced?
3567	What measures are used for evaluation?
3568	what were the baselines?
3569	what datasets were used?
3570	What BERT models are used?
3571	What are the sources of the datasets?
3572	What labels does the dataset have?
3573	Do they evaluate on English only datasets?
3574	What experiments are used to demonstrate the benefits of this approach?
3575	What hierarchical modelling approach is used?
3576	How do co-purchase patterns vary across seasons?
3577	Which words are used differently across ArXiv?
3578	What is future work planed?
3579	What is this method improvement over the best performing state-of-the-art?
3580	Which baselines are used for evaluation?
3581	Did they used dataset from another domain for evaluation?
3582	How is sensationalism scorer trained?
3583	Which component is the least impactful?
3584	Which component has the greatest impact on performance?
3585	What is the state-of-the-art system?
3586	Which datasets are used?
3587	What is the message passing framework?
3588	What other evaluation metrics are looked at?
3589	What is the best reported system?
3590	What kind of stylistic features are obtained?
3591	What traditional linguistics features did they use?
3592	What cognitive features are used?
3593	What approaches do they use towards text analysis?
3594	What dataset do they use for analysis?
3595	Do they demonstrate why interdisciplinary insights are important?
3596	What background do they have?
3597	What kind of issues (that are not on the forefront of computational text analysis) do they tackle?
